---
title: 训练时batch_size大小对收敛的影响
# description: 
date: 2020-01-07
# slug: 
# image: 
categories:
    - 深度学习
---

从下图(a)可以看出，每次迭代选取的批量样本数越多，下降效果越明显，并且取现越平滑。当每次选取一个样本时（相当于随机梯度下降），损失整体是下降趋势，但局部看会来回震荡。
![a](1.png)


从(b)可以看出，如果按整个数据集迭代的来看损失变化情况，则小批量样本数越小，下降效果越明显。
![b](2.png)