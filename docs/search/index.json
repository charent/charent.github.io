[{"content":"大模型下游任务微调时loss出现尖刺处理办法 之前在对大模型进行微调时，loss出现了尖刺，现在想起来就记录一下。\n先说一下下游任务，模型的输入是较长的文本（单个样本约4000 tokens，7000左右的文本长度），输出是特定格式的文本摘要。\n训练主要硬件配置如下：\n1 2 3 CPU：Intel(R) Xeon(R) Gold 5318Y CPU @ 2.10GHz GPU： 2 * 4090 24G 内存：128 GB 基座模型及训练的主要参数如下：\n1 2 3 4 5 6 7 8 base model：Qwen2.5-7B-Instruct-GPTQ-Int8 total batch size: 32 lr: 1e-6 num_train_epochs: 5 lora_rank: 8 lora_alpha：16 lr_scheduler_type：cosine_with_restarts lr_scheduler_num_cycles：4 训练方式：用accelerate的分布式后端FSDP做数据并行DDP，训练代码是二次封装的transformers的Trainer，数据处理部分是自己写的，对输入的system、user部分的token做了屏蔽，只计算模型回复部分assistant部分的loss。然后出现了让广大LLMer头疼的问题：loss尖刺。如下图所示。\n除了第0个epoch，每个epoch的第一个batch都出现loss尖刺，尝试跳过每个epoch的第一个batch、重新打乱数据，问题依然存在。也试过打印第一个batch的数据进行检查，但并没有发现异常。\n后面在网上搜到了这篇博客：Hugging Face Accelerate 两个后端的故事：FSDP 与 DeepSpeed 。省流： FSDP 与 DeepSpeed 在混合精度处理方面有差异，FSDP使用较低的学习率可能会导致不收敛。另外考虑到动态学习率的循环次数num_cycles和num_train_epochs较接近，可能会对loss有影响。故对调整以下参数为新的值：\n1 2 lr: 1e-4 lr_scheduler_num_cycles：8 问题解决：\n","date":"2025-01-12T00:00:00Z","image":"https://charent.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E5%87%BA%E7%8E%B0loss%E5%B0%96%E5%B3%B0/%E5%B0%96%E5%88%BA/1_hu_989d61a643fd7245.png","permalink":"https://charent.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E5%87%BA%E7%8E%B0loss%E5%B0%96%E5%B3%B0/%E5%B0%96%E5%88%BA/","title":"大模型微调出现loss尖峰/尖刺"},{"content":"基于mini hash的文档去重 实现代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 from datasketch import MinHash, MinHashLSH from collections import defaultdict # 结束标点符号 END_PUN = set(\u0026#34;.。!！）)》}】?？\\\u0026#34;”\u0026#34;) # 保留中文和英文、下划线，不要标点符号 NON_CHAR = re.compile(\u0026#34;[^[\\u4E00-\\u9FA5|A-Za-z_0-9]\u0026#34;) def _get_doc_mini_hash(doc: list[str] | str, num_perm: int) -\u0026gt; MinHash: \u0026#39;\u0026#39;\u0026#39; 获取一段文本的mini hash \u0026#39;\u0026#39;\u0026#39; mini_hash = MinHash(num_perm=num_perm) for s in doc: mini_hash.update(s.encode(\u0026#39;utf-8\u0026#39;)) return mini_hash class DropDatasetDuplicate: def __init__(self, threshold: float=0.85, num_perm: int=256) -\u0026gt; None: \u0026#39;\u0026#39;\u0026#39; 获取一个数据集中所有重复（相似的超过threshold）的index，输入为：list[str]，一个str元素为一段文本(doc) 如输入： [a, b, c, d, c, d, e] 返回：{4, 5} (后面两个 c, d 的index) \u0026#39;\u0026#39;\u0026#39; self.similar_index_cluster = defaultdict(set) self.data_lsh = MinHashLSH(threshold=threshold, num_perm=num_perm) self.num_perm = num_perm def add_doc(self, index: object, doc: str,) -\u0026gt; set[int]: \u0026#39;\u0026#39;\u0026#39; 添加文档， index： 文档的索引 doc: 文档本身 \u0026#39;\u0026#39;\u0026#39; # 只保留中文和英文、下划线，不要标点符号 doc = \u0026#39;\u0026#39;.join(NON_CHAR.split(doc)) # doc = [\u0026#39;\u0026#39;.join(t) for t in list(ngrams(doc, 3))] doc_hash = _get_doc_mini_hash(doc, self.num_perm) close_duplicates = self.data_lsh.query(doc_hash) self.data_lsh.insert(index, doc_hash) # 所有相似的doc在similar_index_cluster中的key都是最早出现的idx # 如：data中索引inndex 2, 7, 8, 9, 10, 12 是相似的，则在similar_index_cluster中表现为 {2: {8, 9, 10, 12}} if len(close_duplicates) \u0026gt; 0: min_idx= min(close_duplicates) self.similar_index_cluster[min_idx].add(index) def get_duplicate_indexs(self): \u0026#39;\u0026#39;\u0026#39; 返回所有的重复文档索引 \u0026#39;\u0026#39;\u0026#39; similar_index_cluster = self.similar_index_cluster need_to_remove_idx = set() for key_idx in similar_index_cluster.keys(): need_to_remove_idx |= similar_index_cluster[key_idx] return need_to_remove_idx 使用方法 单进程速度非常慢，需要多进程处理（待补充）\n1 2 3 4 5 6 7 8 9 10 # 先顺序遍历获取哪些行是重复的 for prompt, response in progress.track(zip(parquet_table[\u0026#39;prompt\u0026#39;], parquet_table[\u0026#39;response\u0026#39;]), total=parquet_table.num_rows): row_index += 1 doc = f\u0026#34;{prompt.as_py()}{response.as_py()}\u0026#34; drop_dataset_duplicate.add_doc(index=row_index, doc=doc) row_index = -1 need_to_drop_indexs = drop_dataset_duplicate.get_duplicate_indexs() ","date":"2024-01-17T00:00:00Z","permalink":"https://charent.github.io/p/mini-hash%E6%96%87%E6%A1%A3%E5%8E%BB%E9%87%8D/","title":"Mini hash文档去重"},{"content":"反向传播推导 深度学习神经网络的前向传播大家都很清楚，对我来说，反向传播一直都是一知半解。今天重新复习一下。\n我们以sigmoid函数作为一个计算图节点为例（加法乘法太简单了）。首先，sigmoid函数的定义如下： $$ y = \\frac{1}{1 + e^{-x}} $$ 对$y$求$x$的偏导数： $$ \\begin {aligned} \\frac {\\partial y}{ \\partial x} \u0026amp;= \\frac {-1}{(1 + e^{-x})^{2}} · (-e^{-x}) \\ \u0026amp;= (\\frac {1}{1 + e^{-x}}) ^2 · (e^{-x}) \\ \u0026amp;= \\frac {1}{1 + e^{-x}} · \\frac {e^{-x}}{1 + e^{-x}} \\ \u0026amp;= y · \\frac {1 + e^{-x} - 1}{1 + e^{-x}} \\ \u0026amp;= y · (1 - \\frac {1}{1 + e^{-x}}) \\ \u0026amp;= y · (1 - y) \\end {aligned} $$ 假设sigomid节点前向传播的输出是$y$，反向传播到sigomid节点的数值是$L$，根据链式求导法则，则该节点对$x$的梯度为 $ \\frac {\\partial L}{ \\partial y} · \\frac {\\partial y}{ \\partial x} = \\frac {\\partial L}{ \\partial y} · y · (1 - y) $\n到这里可以写代码了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import math class sigmoid: def __init__(self): self.y = None def forward(self, x: float) -\u0026gt; float: # sigmoid func y = 1 / (1 + math.exp( -x )) self.y = y return y def backward(self, out: float) -\u0026gt; float: dx = out * self.y * (1.0 - self.y) return dx ","date":"2024-01-09T00:00:00Z","permalink":"https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/","title":"深度学习反向传播推导"},{"content":"LLama系列模型 llama系列模型是闭源gpt3.5火爆之后开源的强大经典decoder-only模型，后面诞生的诸多llm多多少少都带有LLama的影子。\nLLama2模型结构的主要改进 1. 将Layer_norm更换为RMS_norm 在NLP模型中，归一化对模型训练过程中防止loss起飞有重要作用。\n经典layer norm计算公式如下： $$ y = \\frac {x - E(x)} {\\sqrt{Var(x) + \\epsilon}} * \\gamma + \\beta $$ $$ E(x) = \\frac {1} {N} \\sum^{N}_{i=0} {x_i} $$\n$$ Var(x) = \\frac {1} {N} \\sum^{N}_{i=0}{(x_i - E(x))^2} $$\n其中,$γ$和$β$是可学习的参数。分母加上一个极小的数$ε$防止分母为0。\nRMS norm其实是layer norm的变体，为了加快计算，省去了求均值的过程，也删除了偏置值$β$。 $$ y = \\frac {x} { \\sqrt {Mean(x^2) + \\epsilon}} * \\gamma $$\n$$ Mean(x^2) = \\frac {1} {N} \\sum^{N}_{i=0}({x_i}^2) $$ $γ$是可学习的参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 class RMSNorm(torch.nn.Module): def __init__(self, dim: int, eps: float = 1e-6): super().__init__() self.eps = eps # ε self.gama = nn.Parameter(torch.ones(dim)) #可学习参数γ ​ def _norm(self, x): # RMSNorm return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) ​ def forward(self, x): output = self._norm(x.float()).type_as(x) return output * self.gama 2. Q在与K相乘之前，先使用RoPE进行位置编码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # 作者：CodeLearner # 链接：https://zhuanlan.zhihu.com/p/649756898 def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0): # 计算词向量元素两两分组以后，每组元素对应的旋转角度 # arange生成[0,2,4...126] freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) # t = [0,....end] t = torch.arange(end, device=freqs.device) # type: ignore # t为列向量 freqs为行向量做外积 # freqs.shape = (t.len(),freqs.len()) #shape (end,dim//2) freqs = torch.outer(t, freqs).float() # type: ignore # 生成复数 # torch.polar(abs,angle) -\u0026gt; abs*cos(angle) + abs*sin(angle)*j freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # complex64 # freqs_cis.shape = (end,dim//2) return freqs_cis ​ def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor): # ndim为x的维度数 ,此时应该为4 ndim = x.ndim assert 0 \u0026lt;= 1 \u0026lt; ndim assert freqs_cis.shape == (x.shape[1], x.shape[-1]) shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)] # (1,x.shape[1],1,x.shape[-1]) return freqs_cis.view(*shape) ​ def apply_rotary_emb( xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # xq.shape = [bsz, seqlen, self.n_local_heads, self.head_dim] # xq_.shape = [bsz, seqlen, self.n_local_heads, self.head_dim//2 , 2] # torch.view_as_complex用于将二维向量转换为复数域 torch.view_as_complex即([x,y]) -\u0026gt; (x+yj) # 所以经过view_as_complex变换后xq_.shape = [bsz, seqlen, self.n_local_heads, self.head_dim//2] xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)) xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)) freqs_cis = reshape_for_broadcast(freqs_cis, xq_) # freqs_cis.shape = (1,x.shape[1],1,x.shape[-1]) # xq_ 与freqs_cis广播哈达玛积 # [bsz, seqlen, self.n_local_heads, self.head_dim//2] * [1,seqlen,1,self.head_dim//2] # torch.view_as_real用于将复数再转换回实数向量, 再经过flatten展平第4个维度 # [bsz, seqlen, self.n_local_heads, self.head_dim//2] -\u0026gt;[bsz, seqlen, self.n_local_heads, self.head_dim//2,2 ] -\u0026gt;[bsz, seqlen, self.n_local_heads, self.head_dim] xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3) xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3) return xq_out.type_as(xq), xk_out.type_as(xk) # 精简版Attention class Attention(nn.Module): def __init__(self, args: ModelArgs): super().__init__() self.wq = Linear(...) self.wk = Linear(...) self.wv = Linear(...) self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * 2) ​ def forward(self, x: torch.Tensor): bsz, seqlen, _ = x.shape xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) # attention 操作之前，应用旋转位置编码 xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) #... # 进行后续Attention计算 scores = torch.matmul(xq, xk.transpose(1, 2)) / math.sqrt(dim) scores = F.softmax(scores.float(), dim=-1) output = torch.matmul(scores, xv) # (batch_size, seq_len, dim) # ...... 3. 引入KV Cache，并采用Group Query Attention 出处见图片水印。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 作者：CodeLearner # 链接：https://zhuanlan.zhihu.com/p/649756898 def mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # qkv projection # when we pass kvcache, n_seq = 1. so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u0026gt; [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u0026gt; [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] 4. 用SiLU激活函数代替RELU/GELU 这里先介绍Swish激活函数，计算公式如下： $$ Swish(x) = x · \\sigma (\\beta x) $$\n$$ \\sigma (x) = \\frac{1} {1 + e^{-x} } $$\n其中，$σ$是Sigmoid函数， $\\beta$ 是一个参数，上图为$β=0.5$时的函数图像。当$β=1$时，Swish函数就是SiLU函数。\n5. 使用分组查询注意力 (Grouped Query Attention, GQA) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 # copied from llama2 def repeat_kv(x: torch.Tensor, n_rep: int) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;torch.repeat_interleave(x, dim=2, repeats=n_rep)\u0026#34;\u0026#34;\u0026#34; bs, slen, n_kv_heads, head_dim = x.shape if n_rep == 1: # MHA return x return ( # MQA / GQA x[:, :, :, None, :] .expand(bs, slen, n_kv_heads, n_rep, head_dim) .reshape(bs, slen, n_kv_heads * n_rep, head_dim) ) class Attention(nn.Module): def __init__(self, args: ModelArgs): super().__init__() self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads model_parallel_size = fs_init.get_model_parallel_world_size() self.n_local_heads = args.n_heads // model_parallel_size self.n_local_kv_heads = self.n_kv_heads // model_parallel_size # 此处 self.n_rep = self.n_local_heads // self.n_local_kv_heads # 此处 几个组 self.head_dim = args.dim // args.n_heads self.wq = ColumnParallelLinear( args.dim, args.n_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wk = ColumnParallelLinear( args.dim, self.n_kv_heads * self.head_dim, # 初始化为单个组内的一份 bias=False, gather_output=False, init_method=lambda x: x, ) self.wv = ColumnParallelLinear( args.dim, self.n_kv_heads * self.head_dim, # # 初始化为单个组内的一份 bias=False, gather_output=False, init_method=lambda x: x, ) self.wo = RowParallelLinear( args.n_heads * self.head_dim, args.dim, bias=False, input_is_parallel=True, init_method=lambda x: x, ) self.cache_k = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() self.cache_v = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() def forward( self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor], ): bsz, seqlen, _ = x.shape xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) self.cache_k = self.cache_k.to(xq) self.cache_v = self.cache_v.to(xq) self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] # repeat k/v heads if n_kv_heads \u0026lt; n_heads # 单个组扩展为完整head keys = repeat_kv(keys, self.n_rep) # (bs, seqlen, n_local_heads, head_dim) values = repeat_kv(values, self.n_rep) # (bs, seqlen, n_local_heads, head_dim) xq = xq.transpose(1, 2) # (bs, n_local_heads, seqlen, head_dim) keys = keys.transpose(1, 2) values = values.transpose(1, 2) scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim) if mask is not None: scores = scores + mask # (bs, n_local_heads, seqlen, cache_len + seqlen) scores = F.softmax(scores.float(), dim=-1).type_as(xq) output = torch.matmul(scores, values) # (bs, n_local_heads, seqlen, head_dim) output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) return self.wo(output) ","date":"2023-11-12T00:00:00Z","permalink":"https://charent.github.io/p/llama2%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E6%96%B9%E9%9D%A2%E7%9A%84%E6%94%B9%E8%BF%9B/","title":"LLama2模型结构方面的改进"},{"content":"问题背景 假设我们有一个很大的数据集（GB级别），需要加载到内存中，但是全部加载的话会导致内存爆炸。同时我们还要保证可以在训练的不同的epoch打乱数据，所以先打乱数据保存到文件，再一行一行（一个样本一个样本）加载进来行不通，而且一行一行加载会导致非常大的IO，速度还会变慢。\n解决办法 先把数据集转换为parquet文件，这中个格式的好处是可以分块读取，有效降低磁盘IO。 在torch数据集类dataset中对parquet文件循环，一次加载N条数据到内存缓冲区中，对缓存中的N条数据执行打乱（shuffle）操作即可 为了实现每个epoch都可以循环区数据集中的数据，可以使用python的yield特性实现迭代。 代码示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 from typing import Union from torch.utils.data import Dataset from torch import LongTensor from transformers import PreTrainedTokenizerFast import pyarrow.parquet as pq from numpy import array, int64 from numpy.random import shuffle class MyDataset(Dataset): def __init__(self, parquet_file: str, tokenizer_dir: str, keep_in_memory: bool=False, max_seq_len: int=512, buffer_size: int=40960, ) -\u0026gt; None: \u0026#39;\u0026#39;\u0026#39; keep_in_memory: 是否将parquet文件转换为pandas.DataFrame格式存放到内存, False将使用迭代生成器(迭代生成器不支持打乱数据)，减少大数据集内存占用 \u0026#39;\u0026#39;\u0026#39; super().__init__() self.keep_in_memory = keep_in_memory self.max_seq_len = max_seq_len # 使用pyarrow.parquet读取，to_pandas、for遍历速度更快 parquet_table = pq.read_table(parquet_file) # 获取数据集长度 self.length = parquet_table.num_rows # 缓冲区大小不能超过数据长度 self.buffer_size = self.length if buffer_size \u0026gt; self.length else buffer_size if keep_in_memory: # 转化为pandas放到内存中 self.data = parquet_table.to_pandas() else: self.data = parquet_table # 初始化tokenizer self.tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_dir) # 在这里初始化generator self.sample_generator = self.item_generator() def item_generator(self,) -\u0026gt; tuple: \u0026#39;\u0026#39;\u0026#39; 一条数据的生成器，防止大数据集OOM \u0026#39;\u0026#39;\u0026#39; parquet_table = self.data # 生成器是死循环，不用退出，训练结束（epoch结束）会停止调用next() buffer_list = [] while True: for prompt, response in zip(parquet_table[\u0026#39;prompt\u0026#39;], parquet_table[\u0026#39;response\u0026#39;]): # 缓存数据不够，添加数据 if len(buffer_list) \u0026lt; self.buffer_size: buffer_list.append( (prompt.as_py(), response.as_py()) ) continue # 执行到这里，缓存区够了，打乱数据 shuffle(buffer_list) for p, r in buffer_list: # 在这里迭代 yield p, r # 迭代完成，清空缓存区 buffer_list = [] def __getitem__(self, index): \u0026#39;\u0026#39;\u0026#39; 返回一条样本 \u0026#39;\u0026#39;\u0026#39; if self.keep_in_memory: data = self.data prompt, response = data.iloc[index].prompt, data.iloc[index].response else: prompt, response = next(self.sample_generator) max_seq_len = self.max_seq_len - 5 # len(\u0026#39;[EOS]\u0026#39;) = 5 # add an eos token note that end of resopnse, using in generate. return f\u0026#34;{prompt[0: max_seq_len]}[EOS]\u0026#34;, f\u0026#34;{response[0: max_seq_len]}[EOS]\u0026#34; def collate_fn(self, data: list[list]) -\u0026gt; dict: \u0026#39;\u0026#39;\u0026#39; 合并一个批次数据返回 \u0026#39;\u0026#39;\u0026#39; tokenizer = self.tokenizer prompt = tokenizer([item[0] for item in data], padding=True, return_token_type_ids=False) response = tokenizer([item[1] for item in data], padding=True, return_token_type_ids=False) input_ids = array(prompt.input_ids, dtype=int64) input_mask = array(prompt.attention_mask, dtype=int64) target_ids = array(response.input_ids, dtype=int64) ret = { \u0026#39;input_ids\u0026#39;: LongTensor(input_ids), \u0026#39;input_mask\u0026#39;: LongTensor(input_mask), \u0026#39;target_ids\u0026#39;: LongTensor(target_ids), } return ret def __len__(self) -\u0026gt; int: return self.length ","date":"2023-11-05T00:00:00Z","permalink":"https://charent.github.io/p/pytorch%E5%8A%A0%E8%BD%BD%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86/","title":"pytorch加载大数据集"},{"content":"一个实现C++快排的算法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 #include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;ctime\u0026gt; using namespace std; #define N 10000000 void rand_nums(int* nums, int n) { srand((unsigned int)time(NULL)); for (int i = 0; i \u0026lt; n; ++i) { nums[i] = rand() % N; } } void quick_sort_sigle_thread(int* nums, int left, int right) { if (left \u0026gt;= right) return; int i = left, j = right, base = nums[left]; while (i \u0026lt; j) { while (i \u0026lt; j \u0026amp;\u0026amp; nums[j] \u0026gt;= base) --j; //找一个比base小的 while (i \u0026lt; j \u0026amp;\u0026amp; nums[i] \u0026lt;= base) ++i; //找一个比base大的 if (i \u0026lt; j) swap(nums[i], nums[j]); } swap(nums[left], nums[i]); quick_sort_sigle_thread(nums, left, i - 1); quick_sort_sigle_thread(nums, i + 1, right); } void quick_sort_multi_thread(int* nums, int left, int right) { if (left \u0026gt;= right) return; int i = left, j = right, base = nums[left]; while (i \u0026lt; j) { while (i \u0026lt; j \u0026amp;\u0026amp; nums[j] \u0026gt;= base) --j; //找一个比base小的 while (i \u0026lt; j \u0026amp;\u0026amp; nums[i] \u0026lt;= base) ++i; //找一个比base大的 if (i \u0026lt; j) swap(nums[i], nums[j]); } swap(nums[left], nums[i]); thread threads[2]; if (right - left \u0026lt;= 100000) { //小于10w用递归 threads[0] = thread(quick_sort_sigle_thread, nums, left, i - 1); threads[1] = thread(quick_sort_sigle_thread, nums, i + 1, right); } else { //大于10w继续用多线程分割 threads[0] = thread(quick_sort_multi_thread, nums, left, i - 1); threads[1] = thread(quick_sort_multi_thread, nums, i + 1, right); } for (int i = 0; i \u0026lt; 2; ++ i) threads[i].join(); } void print_nums(int* nums, int end, int start = 0) { for (int i = start; i \u0026lt; start + end; ++i) { cout \u0026lt;\u0026lt; nums[i] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } cout \u0026lt;\u0026lt; endl; } int main() { int *nums = new int[N]; rand_nums(nums, N); // print_nums(nums, N); cout \u0026lt;\u0026lt; \u0026#34;数组大小：\u0026#34; \u0026lt;\u0026lt; N \u0026lt;\u0026lt; endl; int start_time, end_time; start_time = clock(); quick_sort_sigle_thread(nums, 0, N - 1); end_time = clock(); cout \u0026lt;\u0026lt; \u0026#34;单线程用时：\u0026#34; \u0026lt;\u0026lt; (end_time - start_time) / 1000.0 \u0026lt;\u0026lt; \u0026#39;s\u0026#39; \u0026lt;\u0026lt; endl; // print_nums(nums, N); rand_nums(nums, N); start_time = clock(); quick_sort_multi_thread(nums, 0, N - 1); end_time = clock(); cout \u0026lt;\u0026lt; \u0026#34;多线程用时：\u0026#34; \u0026lt;\u0026lt; (end_time - start_time) / 1000.0 \u0026lt;\u0026lt; \u0026#39;s\u0026#39; \u0026lt;\u0026lt; endl; // print_nums(nums, N); delete[] nums; return 0; } ","date":"2023-10-30T00:00:00Z","permalink":"https://charent.github.io/p/c-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","title":"C++多线程实现快速排序算法"},{"content":"问题 在用dataloadaer加载数据集训练模型时，参数如下：\n1 2 3 4 5 6 7 8 train_dataloader = DataLoader( train_dataset, # 大小为900多万 batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn, pin_memory=True, num_workers=8, ) 在训练过程中CPU内存缓慢增涨，训练一个epoch还没结束，60G内存就OOM了。\n解决 搜索后发现，num_workers \u0026gt;= 2时，会把已经加载的batch数据缓存到CPU内存中，对于小数据集内存占用不明显，但是大数据集内存占用很可观。 设置pin_memory=False和num_workers=0后问题解决，但是num_workers=0会导致数据加载较慢，一个epoch会多消耗约30分钟，目前还没有特别好的解决方法。\n","date":"2023-10-09T00:00:00Z","permalink":"https://charent.github.io/p/pytorch%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86num_workers%E5%A4%A7%E4%BA%8E1%E5%86%85%E5%AD%98%E7%BC%93%E6%85%A2%E5%A2%9E%E6%B6%A8/","title":"Pytorch大数据集num_workers大于1内存缓慢增涨"},{"content":"Parquet文件介绍 Parquet 是 Hadoop 生态圈中主流的列式存储格式，最早是由 Twitter 和 Cloudera 合作开发，2015 年 5 月从 Apache 孵化器里毕业成为 Apache 顶级项目。\n优点：\n数据压缩比高，文件大小较小，适合网络传输; 读写方便， python中pandas支持直接读写，FastParquet和pyarrow则提供更多的自定义操作。 I/O操作次数少，减少磁盘的使用率。 内存占用少，适合处理大数据集。 读Parquet文件 FastParquet读及遍历 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from fastparquet import ParquetFile pf = ParquetFile(\u0026#39;./example.parquet\u0026#39;) # 大数据集to_pandas会占用大量的内存 # df = pf.to_pandas() # 查看行数 print(pf.count()) for pf_chunk in pf: for rows in pf_chunk.iter_row_groups(): for prompt, response in zip(rows[\u0026#39;prompt\u0026#39;], rows[\u0026#39;response\u0026#39;]): pass pyarrow读及遍历 1 2 3 4 5 6 7 8 9 10 11 12 import pyarrow.parquet as pq pt = pq.read_table(\u0026#39;./example.parquet\u0026#39;) # 大数据集to_pandas会占用大量的内存 # df = pt.to_pandas() # 查看行数 print(pt.num_rows) for prompt, response in zip(pt[\u0026#39;prompt\u0026#39;], pt[\u0026#39;response\u0026#39;]): prompt, response = prompt.as_py(), response.as_py() ","date":"2023-08-21T00:00:00Z","permalink":"https://charent.github.io/p/parquet%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%86%99%E5%92%8C%E5%BE%AA%E7%8E%AF%E9%81%8D%E5%8E%86/","title":"Parquet文件的读写和循环遍历"},{"content":"prime模板 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Solution: def minCostConnectPoints(self, points: List[List[int]]) -\u0026gt; int: n = len(points) def get_distance(a: list[int], b: list[int]) -\u0026gt; int: return abs(a[0] - b[0]) + abs(a[1] - b[1]) int_max = 2 ** 31 ans = 0 graph = [[0 for _ in range(n)] for _ in range(n)] for i in range(n): for j in range(n): dist = get_distance(points[i], points[j]) graph[i][j] = dist graph[j][i] = dist low_cost = [int_max for _ in range(n)] vec = [0 for _ in range(n)] vec[0] = 1 # 从顶点0开始 for i in range(1, n): low_cost[i] = graph[0][i] # 顶点0到i的距离 for _ in range(1, n): # 找出利vec中为离所有1的点最近的点 min_idx = -1 min_cost = int_max for j in range(n): # vec[j] = 0没有加入最小生成树合集的节点j if vec[j] == 0 and low_cost[j] \u0026lt; min_cost: min_idx = j min_cost = low_cost[j] vec[min_idx] = 1 ans += min_cost # 更新vec中的所有lowcost for j in range(n): # 初始化时low_cost[j]为0到j的距离，加入新的节点min_idx后，最小生成树到未加入节点的距离可能会变短 # vec[j] = 0没有加入最小生成树合集的节点j，更新j到新加入节点min_idx的最小距离 if vec[j] == 0 and graph[min_idx][j] \u0026lt; low_cost[j]: low_cost[j] = graph[min_idx][j] # print(vec, low_cost) return ans ","date":"2023-05-17T00:00:00Z","permalink":"https://charent.github.io/p/prime%E7%AE%97%E6%B3%95/","title":"Prime算法"},{"content":"并查集实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def find(parent: list, node: int) -\u0026gt; int: while parent[node] != node: parent[node] = parent[parent[node]] node = parent[node] return node def find(parent: list, node: int) -\u0026gt; int: if parent[node] != node: # find的过程中，将每个节点的父节点都设置为最后的节点 parent[node] = find(parent, parent[node]) return parent[node] def union(parent: list, node1: int, node2: int) -\u0026gt; None: parent[find(parent, node1)] = find(parent, node2) ","date":"2023-03-02T00:00:00Z","permalink":"https://charent.github.io/p/%E5%B9%B6%E6%9F%A5%E9%9B%86/","title":"并查集"},{"content":"前缀树字典形式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 构建前缀树 for word in dictionary: cur = trie for ch in word: if ch not in cur: cur[ch] = {} cur = cur[ch] cur[\u0026#39;#\u0026#39;] = {} # word的结束 # 查找 for i, word in enumerate(sentence): cur = trie for j, ch in enumerate(word): if \u0026#39;#\u0026#39; in cur: find = True break if ch not in cur: break cur = cur[ch] 前缀树class形式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Trie { class TrieNode { boolean end; TrieNode[] tns = new TrieNode[26]; } TrieNode root; public Trie() { root = new TrieNode(); } public void insert(String s) { TrieNode p = root; for(int i = 0; i \u0026lt; s.length(); i++) { int u = s.charAt(i) - \u0026#39;a\u0026#39;; if (p.tns[u] == null) p.tns[u] = new TrieNode(); p = p.tns[u]; } p.end = true; } public boolean search(String s) { TrieNode p = root; for(int i = 0; i \u0026lt; s.length(); i++) { int u = s.charAt(i) - \u0026#39;a\u0026#39;; if (p.tns[u] == null) return false; p = p.tns[u]; } return p.end; } public boolean startsWith(String s) { TrieNode p = root; for(int i = 0; i \u0026lt; s.length(); i++) { int u = s.charAt(i) - \u0026#39;a\u0026#39;; if (p.tns[u] == null) return false; p = p.tns[u]; } return true; } } ","date":"2022-11-02T00:00:00Z","permalink":"https://charent.github.io/p/%E5%AD%97%E5%85%B8%E6%A0%91%E5%89%8D%E7%BC%80%E6%A0%91trie/","title":"字典树、前缀树、trie"},{"content":"如果求组合数就是外层for循环遍历物品，内层for遍历背包。 如果求排列数就是外层for遍历背包，内层for循环遍历物品。\n常见的背包问题有1、组合问题。2、True、False问题。3、最大最小问题。 分为三类。 1、组合问题： 377. 组合总和 Ⅳ 494. 目标和 518. 零钱兑换 II 2、True、False问题： 139. 单词拆分 416. 分割等和子集 3、最大最小问题： 474. 一和零 322. 零钱兑换\n组合问题公式 1 dp[i] += dp[i-num] True、False问题公式\n1 dp[i] = dp[i] or dp[i-num] 最大最小问题公式\n1 2 3 dp[i] = min(dp[i], dp[i-num]+1) #或者 dp[i] = max(dp[i], dp[i-num]+1) 以上三组公式是解决对应问题的核心公式。 当然拿到问题后，需要做到以下几个步骤：\n分析是否为背包问题。 是以上三种背包问题中的哪一种。 是0-1背包问题还是完全背包问题。也就是题目给的nums数组中的元素是否可以重复使用。 如果是组合问题，是否需要考虑元素之间的顺序。需要考虑顺序有顺序的解法，不需要考虑顺序又有对应的解法。 接下来讲一下背包问题的判定 背包问题具备的特征：给定一个target，target可以是数字也可以是字符串，再给定一个数组nums，nums中装的可能是数字，也可能是字符串，问：能否使用nums中的元素做各种排列组合得到target。 背包问题技巧： 1.如果是0-1背包，即数组中的元素不可重复使用，nums放在外循环，target在内循环，且内循环倒序；\n1 2 for num in nums: for i in range(target, nums-1, -1): 2.如果是完全背包，即数组中的元素可重复使用，nums放在外循环，target在内循环。且内循环正序。\n1 2 for num in nums: for i in range(nums, target+1): 3.如果组合问题需考虑元素之间的顺序，需将target放在外循环，将nums放在内循环。\n1 2 for i in range(1, target+1): for num in nums: 数组不变，区间查询：前缀和、树状数组、线段树； 数组单点修改，区间查询：树状数组、线段树； 数组区间修改，单点查询：差分、线段树； 数组区间修改，区间查询：线段树。\n","date":"2022-08-14T00:00:00Z","permalink":"https://charent.github.io/p/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","title":"动态规划"},{"content":"前言 IV值（Information Value），信息价值指标，是评价一个特征好不好的指标之一。在金融风控领域广泛应用，尤其是在特征选择的场景下，会经常使用这个指标，特征选择得好不好，将直接影响模型的效果。\n在金融风控领域，我们处理的更多是二分类问题，即是判断一个账号黑账户还是白账户。风险识别模型的效果很大程度上取决于对黑账户特征的分析，分析的黑账户越多，特征经验越丰富，模型效果越好。但是如何挑选分析得到的特征呢？用什么样的标准去判断这个特征能不能用呢？最方便的方法当然是拿所有的特征去训练一个模型，看看特征重要性，但这不是本文讨论的内容，我们要在特征做完之后，立刻判断这个特征能不能用，而不是等所有特征做完再去看特征重要性。用IV值！\nIV值先验知识1：分箱 特征分箱主要是为了降低变量的复杂性和减少变量噪音对模型的影响，提高自变量和因变量的相关度，从而使模型更加稳定。\n监督学习中的分箱常用的有Split分箱和Merge分箱。\nSplit 分箱是一段连续的值分为若干段离散的值，Split分箱和决策树比较相似，切分点的选择指标主要有信息熵、gini 指数等。比如，年龄可以分为 ≤18，19-24，25-35，35-54，≥55。 Merge分箱则通过计算两个从小到大排序的数值的卡方值，将最小卡方值的相邻组合并为一组，再重新排序，重新计算卡方值，合并组，直到计算出的卡方值都不低于事先设定的阈值, 或者分组数达到一定的数量。 IV值先验知识2：WOE编码 分箱之后我们便得到了一系列的离散变量，接下来需要对变量进行编码，将离散变量转化为连续变量。WOE编码是评分卡模型常用的编码方式。\nWOE称为证据权重(weight of evidence)，是一种有监督的编码方式，将预测类别的集中度的属性作为编码的数值。对于自变量第 $i$ 箱的WOE值为： $$ WOE_i = ln(\\frac{P_{i_1}} {P_{i_0}} ) = ln(\\frac {B_i / B_T} { G_i / G_T }) $$ 其中:\n$P_{i_1}$ 是第 $i$ 箱中黑账户占所有黑账户比例；\n$P_{i_0}$ 是第 $i$ 箱中白账户占所有白账户的比例；\n$B_i$ 是第 $i$ 箱中黑账户的个数；\n$G_i$ 是第 $i$ 箱中白账户的个数；\n$B_T$ 是所有黑账户个数；\n$G_T$ 是所有白账户个数。\n变换以后可以看出，WOE也可以理解为当前分箱中黑账户和白账户的比值，和所有样本中这个比值的差异。WOE越大，这种差异越大，当前分组里的黑账户的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。当分箱中黑账户和白账户的比例等于样本中所有黑账户和白账户的比值时，说明这个分箱没有预测能力，即WOE=0。\nQA：为什么不直接使用原始数据中的连续变量，而是先分箱为离散变量，再将离散变量转换为连续变量WOE编码？ WOE可以把相对于预测变量显现非线性的特征转换为线性。例如：很多黑账户的年龄在19-24岁，随着年龄的增长黑账户逐渐变小，黑账户数量（纵坐标，数值为黑账户占总账户比）和年龄是一个非线性的关系。分箱后转换为WOE编码，黑账户数量（纵坐标，数值为黑账户占总账户比）和WOE值呈线性关系。如下图所示，对于机器学习模型而言，线性关系更容易区分黑白账户。\nIV值计算 特征变量中第 $i$ 个分箱对应的IV值的计算公式为： $$ IV_i = ( \\frac{B_i} {B_T} - \\frac{G_i} {G_T} ) \\times ln(\\frac {B_i / B_T} { G_i / G_T}) \\ = ( \\frac{B_i} {B_T} - \\frac{G_i} {G_T} ) \\times WOE_i $$\n变量中第 $i$ 个分箱对应的IV值的计算公式为： $$ IV = \\sum \\limits _{i = 1}^n IV_i $$\n特别地，如果特征没有进行分箱操作，相当于只有一个像，上面公式的i和i都等于1。 IV值的取值范围是[0,+∞)，当分箱中只包含白账户或只包含黑账户时，IV = +∞，当分箱中黑白账户比例等于整体黑白账户比例时，IV为0。\nIV值计算完成后，即可根据IV值的大小判断特征是否对有用（特征的预测能力是否强）。\nIV值范围 预测能力 ＜0.02 无效特征，无预测能力 [0.02, 0.10) 弱效果特征，预测能力弱 [0.10, 0.50) 有效特征，预测能力中等 ≥0.50 强特征，预测能力强 ","date":"2022-04-16T00:00:00Z","permalink":"https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/","title":"机器学习特征选择之IV值计算"},{"content":"方法 使用三列的表格实现公式居中 使用制表位实现公式居中 首先确定纸张大小和页边距： A4：21x29.7 厘米 页边距：一般是上下：2.54厘米，左右：3.18厘米 第一个制表位：设置在页面中间，计算公式为: $$ (21cm - 3.18cm × 2) ÷ 2 = 7.32 cm $$ 第二个制表位：设置在页面右侧，计算公式为: $$ 21cm - 3.18cm x 2 = 14.64 cm $$ 新建一个样式，修改样式，把首行缩进设置为0，点击左下角的格式，选择制表位。 设置第一个制表位，居中对齐: 设置第二个制表位，居右对齐: 在编辑公式时应用新样式即可\n","date":"2021-02-26T00:00:00Z","permalink":"https://charent.github.io/p/office%E5%85%AC%E5%BC%8F%E5%B1%85%E4%B8%AD/","title":"office公式居中"},{"content":"##dd\n","date":"2021-02-26T00:00:00Z","permalink":"https://charent.github.io/p/%E5%88%B7%E9%A2%98%E6%A8%A1%E6%9D%BF/","title":"刷题模板"},{"content":"创建两个密钥 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Linux cd ~/.ssh # Windows cd C:/Users/Dream/.ssh/ # 生成gitee密钥 # 创建gitee的ssh key文件名输入id_rsa.gitee ssh-keygen -t rsa -C \u0026#39;你的邮箱@163.com\u0026#39; # 生成github密钥 # 创建github的ssh key文件名输入id_rsa.github ssh-keygen -t rsa -C \u0026#39;你的邮箱@163.com\u0026#39; 最后在.ssh目录下得到如下4个文件：\nid_rsa.gitee id_rsa.gitee.pub id_rsa.github id_rsa.github.pub 把公钥（id_rsa.*.pub）复制到Gitee、Github 这一步比较简单，去自己的git页面的设置找就可以了\n配置密钥使用config 在·目录下创建config文件，添加以下内容到文件中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # gitee Host gitee.com HostName gitee.com User 你的用户名 PreferredAuthentications publickey # Linux：IdentityFile ~/.ssh/id_rsa.gitee IdentityFile C:\\Users\\XXX\\.ssh\\id_rsa.gitee # github Host github.com HostName github.com User 你的用户名 PreferredAuthentications publickey # Linux: IdentityFile ~/.ssh/id_rsa.github IdentityFile C:\\Users\\XXX\\.ssh\\id_rsa.github 测试密钥是否配置成功 1 2 3 4 5 6 7 8 9 10 11 12 # gitee测试 ssh -T git@gitee.com # 返回以下信息表示配置成功 Hi XXX! You\u0026#39;ve successfully authenticated, but GITEE.COM does not provide shell access. # github测试 ssh -T git@github.com # 返回以下信息表示配置成功 Hi XXX! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. 配置一个项目可以推送两个仓库 在项目的.git/config文件修改url = git@github.com:你的用户名/你的项目仓库.git，如果你要推送到多个git仓库，比如要推送到gitee和gitub，则配置[remote \u0026quot;github\u0026quot;]和[remote \u0026quot;gitee\u0026quot;]\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true # 修改GitHub仓库地址为ssh推送 [remote \u0026#34;github\u0026#34;] url = git@github.com:你的用户名/你的项目仓库.git.git fetch = +refs/heads/*:refs/remotes/github/* # 修改Gitee仓库地址为ssh推送 [remote \u0026#34;gitee\u0026#34;] url = git@gitee.com:你的用户名/你的项目仓库.git fetch = +refs/heads/*:refs/remotes/gitee/* [branch \u0026#34;main\u0026#34;] remote = gitee merge = refs/heads/main ","date":"2021-01-03T00:00:00Z","permalink":"https://charent.github.io/p/%E9%85%8D%E7%BD%AE%E9%A1%B9%E7%9B%AE%E5%8F%AF%E6%8E%A8%E9%80%81%E5%88%B0github%E5%92%8Cgitee/","title":"配置项目可推送到github和gitee"},{"content":"前序遍历 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 //前序遍历 /* a / \\ b c a b c */ vector\u0026lt;int\u0026gt; preorder(TreeNode* root) { stack\u0026lt;TreeNode*\u0026gt; s; TreeNode* p = root; vector\u0026lt;int\u0026gt; ans; while (p || !s.empty()) { //左子树走到底 while (p) { //访问根节点 ans.push_back(p-\u0026gt;val); s.push(p); p= p-\u0026gt;left; } //走完左子树，该走右子树了 if (!s.empty()) { p = s.top(); s.pop(); //遍历右子树 p = p-\u0026gt;right; } } return ans; } 中序遍历 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 //中序遍历 /* a / \\ b c b a c */ vector\u0026lt;int\u0026gt; preorder(TreeNode* root) { stack\u0026lt;TreeNode*\u0026gt; s; TreeNode* p = root; vector\u0026lt;int\u0026gt; ans; while (p || !s.empty()) { //左子树走到底 while (p) { s.push(p); p = p-\u0026gt;left; } //走完左子树，访问 if (!s.empty()) { p = s.top(); s.pop(); //访问 ans.push_back(p-\u0026gt;val); //遍历右子树 p = p-\u0026gt;right; } } return ans; } 后序遍历 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 //后序遍历 /* a / \\ b c b c a */ vector\u0026lt;int\u0026gt; preorder(TreeNode* root) { stack\u0026lt;TreeNode*\u0026gt; s; TreeNode* p = root; TreeNode* last_visit = nullptr;//最后一个访问的节点,初始化为null vector\u0026lt;int\u0026gt; ans; while (p || !s.empty()) { if (p) { //p不为空，走到左子树的最末尾 s.push(p); p = p-\u0026gt;left; } else //左子树已经走到底 { p = s.top(); //取栈顶元素，判断该怎么走 if (p-\u0026gt;right \u0026amp;\u0026amp; p-\u0026gt;right != last_visit) { //右子树存在，且不是上次访问的节点 p = p-\u0026gt;right; } else //右子树已经为空或者已经访问 { //访问根节点 ans.push_back(p-\u0026gt;val); last_visit = p; s.pop(); //访问过的节点出栈 p = nullptr;//设置p为空，让循环再次取出栈顶元素 } } } return ans; } ","date":"2020-10-27T00:00:00Z","permalink":"https://charent.github.io/p/cpp%E4%BA%8C%E5%8F%89%E6%A0%91%E9%9D%9E%E9%80%92%E5%BD%92%E9%81%8D%E5%8E%86/","title":"CPP二叉树非递归遍历"},{"content":" lock_guard\u0026lt;mutex\u0026gt; lock(my_mutex) 它将尝试获取提供给它的互斥锁的所有权。当控制流离开lock_guard对象的作用域时，lock_guard析构并释放互斥量;\nunique_lock \u0026lt;mutex\u0026gt; lock(my_mutex)，是 lock_guard 的升级加强版，它具有 lock_guard 的所有功能，同时又具有其他很多方法，可以随时加锁解锁，能够应对更复杂的锁定需要；条件变量需要该类型的锁作为参数时必须使用unique_lock\n条件变量（condition_variable）唤醒阻塞线程的方式： notify_all()：唤醒了等待的所有线程，但是这些被唤醒的线程需要去竞争锁，获取锁之后才能执行 notify_one()：唤醒的顺序是阻塞的顺序，先阻塞先唤醒，没有竞争\n示例1 1 2 3 4 5 6 7 8 9 10 //mutex:互斥量，互斥量:为协调共同对一个共享资源的单独访问而设计的 //condition_variable:、信号量(条件变量),为控制一个具有 有限数量用户 的资源而设计 //condition_variable一般和互斥量一起使用 mutex mtx; //手动上锁、解锁，lock_guard 和 unique_lock 是自动上锁和解锁 mtx.lock(); //代码段 ...直接使用mutex的加锁功能，能实现简单的互斥， ...mtx被其他线程lock之后，当前线程会阻塞，直到其他线程unlock，所有的阻塞进程会竞争该锁 ... mtx.unlock() 示例2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 #include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;mutex\u0026gt; #include \u0026lt;deque\u0026gt; #include \u0026lt;condition_variable\u0026gt; #include \u0026lt;atomic\u0026gt; using namespace std; //是否让线程循环的全局变量，使用原子类型，保证多线程下，同一时刻只有一个线程操作该变量 //volatile让线程每次需要存储或读取这个变量的时候，都会直接从变量地址中读取数据 //而不是从编译器优化后的寄存器中读取，防止出现不一致的情况 volatile atomic_bool loop(true); class MsgList { private: deque\u0026lt;int\u0026gt; msg_queue; mutex mtx; condition_variable cond_var; public: void notify_all() { cond_var.notify_all(); } void write_msg() { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); //锁的作用域：整个函数，函数返回后自动释放 for (size_t i = 0; i \u0026lt; 5; ++i) { msg_queue.push_back(i); } //全部数据写入后，唤醒一个阻塞的线程 //notify_all()唤醒了等待的所有线程，但是这些被唤醒的线程需要去竞争锁，获取锁之后才能执行 //这里用notify_one()，唤醒的顺序是阻塞的顺序，先阻塞先唤醒，后阻塞后唤醒，没有竞争 cond_var.notify_one(); } /* lock_guard对象时，它将尝试获取提供给它的互斥锁的所有权。 当控制流离开lock_guard对象的作用域时，lock_guard析构并释放互斥量; unique_lock 是 lock_guard 的升级加强版，它具有 lock_guard 的所有功能， 同时又具有其他很多方法，可以随时加锁解锁，能够应对更复杂的锁定需要; condition_variable 条件变量需要该类型的锁作为参数，必须使用unique_lock */ void read_msg() { /* 在这个例子中，如果不用condition_variable，下面的代码是等价的： void read_msg(){ while (loop){ //死循环 unique_lock\u0026lt;mutex\u0026gt; lock(mtx); //加锁 while (!msg_queue.empty()){ //不为空 //do something } } } 这种写法存在的问题是： 当写线程没有数据的写入的时候（队列始终为空），所有的读线程会一直循环加锁、检测队列是否为空，造成CPU资源浪费 所以就有了下面的条件变量 */ while (loop) { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); //实现对{}内的代码段实现能自动上锁和自动解锁，look是自定义的变量名称，在离开作用域时会析构（既是解锁） cond_var.wait(lock, [this]() -\u0026gt; bool { return !msg_queue.empty() || !loop; } ); //调用wait函数： //1.如果只有第一个参数，先解锁lock，然后将当前线程添加到当前condition_variable对象的等待线程列表中 //当前线程将继续阻塞，直到另外某个线程调用 notify_* 唤醒了当前线程，wait函数自动重新上锁并返回，继续执行下面的代码 //2.如果有第二个参数（是一个返回值为bool的可调用对象，这里用匿名函数） //先解锁、阻塞和1是一样的，当前线程被其他线程唤醒后，再判可调用对象的返回值(bool),如果为fals, //如果返回true（这里是队列不为空返回true），wait函数自动重新上锁并返回，继续执行下面的代码 //这里的!loop是判断是否要退出循环的，loop为false， !lopp为true，则会执行下面的break代码，退出死循环，结束线程 //在第二个参数中，还可以判断读线程的个数，既是允许最大的读线程个数（限制资源的用户量） //wait函数总结就是，先解锁和阻塞，等唤醒；如果有第二个参数，被唤醒后，再判断第一个参数的返回值来决定继续阻塞，还是wait函数返回并执行下面的代码 if (!loop) break; cout \u0026lt;\u0026lt; endl\u0026lt;\u0026lt; \u0026#34;thread id: \u0026#34; \u0026lt;\u0026lt; this_thread::get_id() \u0026lt;\u0026lt; \u0026#34; ; 出队元素:\u0026#34;; while (!msg_queue.empty()) //将队列元素全部出队 { int tmp = msg_queue.front(); msg_queue.pop_front(); cout \u0026lt;\u0026lt; tmp \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } } } MsgList(/* args */){}; ~MsgList(){}; }; int main() { MsgList mlist; int read_thread_num = 5; //5个读线程 thread read_threads[read_thread_num]; for (int i = 0; i \u0026lt; read_thread_num; ++i) { read_threads[i] = thread(\u0026amp;MsgList::read_msg, \u0026amp;mlist); //第一个参数是函数指针（函数的地址），第二个参数是函数的参数，这里是一个实例化的对象的地址 //让这个线程去执行这个实例化的对象的函数 } int write_thread_num = 50; //50个写线程，一次性的 thread write_threads[write_thread_num]; for (int i = 0; i \u0026lt; write_thread_num; ++i) { write_threads[i] = thread(\u0026amp;MsgList::write_msg, \u0026amp;mlist); // write_threads[i].join(); this_thread::sleep_for(chrono::milliseconds(20)); //主线程休眠20ms } for (int i = 0; i \u0026lt; write_thread_num; ++i) write_threads[i].join(); //等待所有的写线程结束 loop = false; //设置loop为false， 退出循环 mlist.notify_all(); //唤醒所有的阻塞的线程，退出循环,结束线程 for (int i = 0; i \u0026lt; read_thread_num; ++i) read_threads[i].join(); //等待所有的读线程结束 return 0; } ","date":"2020-10-09T00:00:00Z","permalink":"https://charent.github.io/p/c-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%8F%8A%E5%90%8C%E6%AD%A5/","title":"C++多线程及同步"},{"content":"纯虚函数 包含纯虚函数的类被称为抽象类。 抽象类中声明的纯虚函数必须在派生类中全部实现，否则编译不会通过。 抽象类无法声明对象。 仅含有纯虚函数而不含有任何其他成员函数和成员变量的类就叫做接口类。 多进程 fork: 子进程读的时候不复制父进程的数据，写的时候复制 vfork:子进程和父进程共享数据，不复制 参考：https://segmentfault.com/a/1190000003745529\nNT_MAX,INT_MIN数值大小 因为int占4字节32位，根据二进制编码的规则，INT_MAX = 2^31-1，INT_MIN= -2^31.C/C++中，所有超过该限值的数，都会出现溢出，出现warning，但是并不会出现error。如果想表示的整数超过了该限值，可以使用长整型long long 占8字节64位。\n由于二进制编码按原码、补码和反码的规则进行运算，所有程序中对INT_MAX和INT_MIN的运算应当格外注意，在出现溢出的时候，不遵循数学规则。\n1 2 3 4 5 INT_MAX + 1 = INT_MIN INT_MIN - 1 = INT_MAX abs(INT_MIN) = INT_MIN ","date":"2020-09-27T00:00:00Z","permalink":"https://charent.github.io/p/cpp%E5%9F%BA%E7%A1%80/","title":"CPP基础"},{"content":" python官网下载对应系统的安装源码包 安装依赖 1 2 3 4 5 6 7 8 9 sudo apt-get install gcc sudo apt-get install make sudo apt-get install zlib* sudo apt-get install libssl-dev sudo apt-get install openssl sudo apt-get install libffi-dev sudo apt-get install sqlite3 sudo apt-get install libsqlite3-dev sudo apt-get install libbz2-dev 安装python37 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 wget https://www.python.org/ftp/python/3.7.7/Python-3.7.7.tgz #解压 tar -zxvf Python-3.7.7.tgz -C /usr/lib/python cd /usr/lib/python/Python3.7.7 sudo gedit ./Modules/Setup #编辑Setup这个文件 # Socket module helper for SSL support; you must comment out the other # socket line above, and possibly edit the SSL variable: #SSL=/usr/local/ssl _ssl _ssl.c \\ -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \\ -L$(SSL)/lib -lssl -lcrypto #去掉上面四行前面的#号 #配置,加上--with-ssl，否则pip3用不了； #加上--enable-optimizations性能大能提升10%,但是可能无法编译 sudo ./configure --with-ssl --enable-loadable-sqlite-extensions sudo make sudo make install ","date":"2020-05-25T00:00:00Z","permalink":"https://charent.github.io/p/%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85python3.7/","title":"源码安装Python3.7"},{"content":"负载均衡、反向代理配置 反向代理指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给Internet上请求连接到客户端。\n安装依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #安装依赖 sudo apt-get install libpcre3 libpcre3-dev apt-get install zlib1g-dev apt-get install openssl #安装nginx sudo apt-get install nginx #启动服务 sudo /etc/init.d/nginx start #编辑配置文件 cd /etc/nginx/sites-available/ vi default 负载均衡将前端的请求根据服务器的负载情况分发给不同的服务器。\n配置文件示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #default 文件内容 #多服务器负载均衡配置 upstream nlp_api { #ip_hash每个请求按照访问ip的hash结果分配 #这样每个访客固定访问一个后端服务器， #可以解决session的问题 #ip_hash; #后面的权重是指定轮询几率 #weight和访问比率成正比，用于后端服务器性能不均的情况 #weight越大，负载的权重就越大,默认1 server 127.0.0.1:8094 weight=5; server 127.0.0.1:8095 weight=5; } server { listen 80; server_name localhost; location / { #不允许跳转 proxy_redirect off; #请求头设置 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header REMOTE-HOST $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #从监听端口进来的请求将由upstream做分发 proxy_pass http://nlp_api; } } 重启服务 1 2 3 4 5 6 7 #检查配置文件是否有问题 sudo nginx -t #最后重新加载配置 sudo /etc/init.d/nginx reload #或者重启服务 sudo /etc/init.d/nginx restart ","date":"2020-03-26T00:00:00Z","permalink":"https://charent.github.io/p/ubuntu%E9%85%8D%E7%BD%AEnginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/","title":"Ubuntu配置nginx负载均衡、反向代理"},{"content":" 1 2 3 4 5 6 7 8 9 wsl.exe --list --running #导出 wsl --export Ubuntu-18.04 D:\\backup\\ubuntu1804.tar #导入 wsl.exe --import Ubuntu-18.04 D:\\ubuntu\\Ubuntu-18.04 D:\\backup\\ubuntu1804.tar #运行 wsl --distribution Ubuntu-18.04 #删除 wsl.exe --unregister Ubuntu-18.04 ","date":"2020-03-26T00:00:00Z","permalink":"https://charent.github.io/p/win10%E4%B8%8Blinux%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/","title":"win10下Linux子系统导入导出"},{"content":"gevent gevent是python的一个并发框架，以微线程greenlet为核心，使用了epoll事件监听机制以及诸多其他优化而变得高效。而且其中有个monkey类，将现有基于Python线程直接转化为greenlet(类似于打patch) 当一个greenlet遇到IO操作时，比如访问网络/睡眠等待，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行。由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO。同时也因为只有一个线程在执行，会极大的减少上下文切换的成本。\ngunicorn Gunicorn是一个unix上被广泛使用的高性能的Python WSGI UNIX HTTP Server。和大多数的web框架兼容，并具有实现简单，轻量级，高性能等特点.\nuvicorn uvicorn 是一个基于 asyncio 开发的一个轻量级高效的 web 服务器框架。仅支持 python 3.5.3 以上版本。\n","date":"2020-03-24T00:00:00Z","permalink":"https://charent.github.io/p/geventgunicornuvicorn%E4%BB%8B%E7%BB%8D/","title":"gevent、gunicorn、uvicorn介绍"},{"content":"通过设置app.run()的参数，来达到多线程的效果，具体参数： 1 2 3 4 5 # 1.threaded : 多线程支持，默认为False，即不开启多线程; app.run(threaded=True) # 2.processes：进程数量，默认为1. app.run(processes=True) #ps：多进程或多线程只能选择一个，不能同时开启 使用genvent做协程，解决高并发： 1 2 3 4 5 6 7 8 9 10 from genvent.wsgi import WSGIServer from genvent import monkey monkey.patch_all() app = Flask(__name__) app.config.from_object(config) api = Api(app) db = DBInfo() # db_old = DBInfo_old() 通过uvcorn(with genvent)的形式来对app进行包装，来启动服务： 1 2 # 启动命令 gunicorn -c gun.py thread_explore:app 其中gun.py是gunicorn的配置文件 thread_explore是服务的主程序 app是flask的app gun.py的具体内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import os import gevent.monkey gevent.monkey.patch_all() import multiprocessing # 服务地址（adderes:port） bind = 127.0.0.1;5000 # 启动进程数量 workers = multiprocessing.cpu_count() * 2 +1 worker_class = \u0026#39;gevent\u0026#39; threads = 20 preload_app = True reload = True x_forwarded_for_header = \u0026#39;X_FORWARDED-FOR\u0026#39; ","date":"2020-03-23T00:00:00Z","permalink":"https://charent.github.io/p/flask%E9%AB%98%E5%B9%B6%E5%8F%91%E5%A4%84%E7%90%86/","title":"Flask高并发处理"},{"content":"孪生网络（Siamese Network） 本质上还是一个网络，但是每次都是两个样本输入到网络中，再计算网络两个输出的相似度。数据集划分为：训练集，支持集，测试集。训练集：从同一类别下采样相同的两个句子作为正样本，从不同的类别下采样两个句子作为负样本，保证正负样本对的数量接近1：1，然后输入到孪生网络中作为一个二分类的任务来度量两个句子之间的距离。\nInduction Network（感应网络） 训练集中，每一个episode的时候，都随机选择C个类（训练集中的类别个数大于C），然后每一个类别都同样随机选择K个样本，这样每一个episode中的数据样本个数便是C * K个，这CK个样本组成support set S，此外，再从剩余的样本中随机选择n个样本作为query set Q，每一个episode都在这样选择出来的S和Q上进行训练 网络由三个模块组成：编码器模块，归纳模块和关系模块 编码器模块： 相当于一个encoder。可以利用CNN，LSTM和Transformer等等，在阿里的论文《Few-Shot Text Classification with Induction Network》中论使用LSTM，简单讲就是：针对每一个样本，将LSTM各个时刻的隐层输出h，做一次self-attention，最后得到一个向量e。\n归纳模块： 借用了胶囊网络的动态路由概念，将每一个类别中的样本表征，最后转化凝练成为class-level的表征。 关系模块： 在归纳模块生成类向量C^i并且查询集中的每个查询文本被编码器模块编码为查询向量e^q之后，下一步就是计算每对查询向量和类向量之间的相关性，输出区间在[0,1]之间的得分\n原型网络（Prototypical Network） 论文《Prototypical Networks for Few-shot Learning》 给定一个训练时的train set，测试时的support set和query。support set 包含C个类别，每个类别下含有K个样本。train set 包含M个类别，每个类别下含有N个样本。为了在训练时期模拟测试时的场景，我们在训练时构造一系列的episode，每个episode实际上就是一个meta task。那该怎么构造这样一个episode呢？从train set中随机抽取C个类别，然后从每个类别中随机抽取K个样本，构造训练时期的support set，这样的问题也称为C-way K-shot问题，接着从另外N-K个样本中选取n个样本作为训练时期的query。构造一系列这样的episode来训练网络\n关系网络（Relation Network） 论文《Learning to Compare: Relation Network for Few-Shot Learning》 整个训练和预测时的方法和原型网络是一样的。其主要创新点在于之前的网络都会给定一个确定的距离度量函数，然而作者认为没有一个确定的距离函数能作为所有类别的最佳度量函数，因此作者让网络自己去学习一个这样的度量函数，这里的Relation network就是通过关系网络来度量query和各类别之间的关系\n","date":"2020-03-15T00:00:00Z","permalink":"https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/","title":"小样本学习"},{"content":"孪生网络之小样本学习： DL标准分类方法：输入x通过多层神经网络后，输出x属于某一类（或多个类）的概率。 小样本分类方法：每个类只需要为数不多的训练样本，要求要识别的样本与训练样本相似，如人脸识别。 孪生网络 孪生网络和学习输入的特征进行分类的网络不同，孪生网络在两个输入中进行区分，即是学习两个输入的相似度。\n孪生网络由两个完全相同的神经网络组成，每个都采用两个输入图像中的一个。然后将两个网络的最后一层馈送到对比损失函数，用来计算两个图像之间的相似度。它具有两个姐妹网络，它们是具有完全相同权重的相同神经网络。图像对中的每个图像将被馈送到这些网络中的一个。使用对比损失函数优化网络（我们将获得确切的函数）\n损失函数： 使用和普通神经网络所不同的损失函数：对比损失函数（Contrastive Loss Function） 孪生架构的目的不是对输入特征进行分类，而是区分它们。因此，分类损失函数（如交叉熵）不是最合适的选择。相反，这种架构更适合使用对比函数。一般而言，这个函数只是评估网络区分一对给定的图像的效果如何。 $$ L = \\frac{1} {2N} \\sum_{i=1}^{n} yd^2 + (1-y)max(margin-d, 0)^2 $$\n其中$d=||an−bn||_2$，代表两个样本特征的欧氏距离，y为两个样本是否匹配的标签，y=1代表两个样本相似或者匹配，y=0则代表不匹配，margin为设定的阈值。 欧氏距离d： $$ d=\\sqrt{(G_w(x_1) - G_w(x_2))^2} $$ 其中$G_w$是其中一个姐妹网络的输出。$X1$和$X2$是输入数据对。 说明： Y值为1或0。如果模型预测输入是相似的，那么Y的值为0，否则Y为1。 max（）是表示0和m-Dw之间较大值的函数。 m是大于0的边际价值（margin value）。有一个边际价值表示超出该边际价值的不同对不会造成损失。这是有道理的，因为你只希望基于实际不相似对来优化网络，但网络认为是相当相似的。\n处理多分类、少样本的问题： 训练数据要保证基本正、负样本比例接近1:1 数据集划分：训练集，测试集，支持集（Support Set），其中支持集包含所有类的样本\n训练： x1、x2可以是正样本、负样本\n样本x1通过网络得到输出y1 样本x2通过网络得到输出y2 使用y1、y2计算对比损失 反向传播计算梯度 使用优化器更新权重 训练集中，x1、x2和标签y的对应关系 x1和x2的关系 y x1、x2属于同一类 0 x1、x2属于不同类 1 测试： 给定测试样本 x ，从支持集中依次取出每个类 x_i（ i=1,2,3,\u0026hellip;.n ），x 和所有 x_i 依次通过孪生网络，若 x_j 和 x 的相似度最高，则认为 x 属于第 j 类（即 x_j 所属的类）\n实验： 数据： 新闻标题数据集，14个类。小样本：每个类中随机抽取50条数据做为训练数据。\n训练集：每个类的50条样本和自身做笛卡尔乘积，得到 50 x 50 = 2500 条正样本，即这2500条样本（x1,x2）对应的y=1；负样本：对每一类，从其余类的各50个样本中，每个类随机抽取 50 / 14 = 4个样本，共56个样本，和该类的50个样本做笛卡尔积组成 50 x 56 = 2800个负样本，即是(x1, x2)对应的 y=0，x1为该类样本，x2为其他类样本。共71400条训练数据 测试集：每个类随机选取3000条，共42000条数据。\n支持集：支持集的选定较为困难。孪生网络在人脸识别中取得的效果非常好，网络训练好之后，直接拿一张人脸照片就可以作为支持集，支持集的样本和测试集的样本输入孪生网络后，网络会输出这两个样本的相似度，再根据相似度判断测试样本和支持样本是否属于同一个类。具体支持集的选取会在实验部分讨论。\n字向量： 使用开源的基于人民日报语料、Word + Ngram训练的预训练词向量，包含1664K的字、词。每个字、词的维度是300。向量矩阵大小：972M\n模型： 1 2 3 4 5 6 7 SiameseClassifier( Conv1D(filters=128, kernel_size=4, strides=2, activation=\u0026#39;relu\u0026#39;) Conv1D(filters=64, kernel_size=3, strides=2, activation=\u0026#39;relu\u0026#39;) Dense(64, activation=\u0026#39;relu\u0026#39;) Dense(32, activation=\u0026#39;relu\u0026#39;) Dense(16, activation=\u0026#39;sigmoid\u0026#39;) ) 模型最后输出一个长度为16的一维数组，用于计算相似度。\n损失函数： 使用对比损失函数，描述两个输出向量之间的欧氏距离的损失。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #对比损失函数 class ContrastiveLoss(tf.keras.losses.Loss): def __init__(self, margin=2.0): \u0026#39;\u0026#39;\u0026#39; margin为边界值 \u0026#39;\u0026#39;\u0026#39; super().__init__() self.margin = margin def call(self, y_true, y_pred): \u0026#39;\u0026#39;\u0026#39; output1和output2为二维数组:(batch_size, dim) \u0026#39;\u0026#39;\u0026#39; output1 = y_pred[0] output2 = y_pred[1] label = tf.cast(y_true,dtype=tf.float32) #计算欧氏距离 d = sqrt(sum(pow((x - y), 2)) d_eu = tf.sqrt(tf.reduce_sum(tf.square(output1 - output2), axis=1,keepdims=False)) #计算对比损失： #loss= Y * (Dw)^2 / 2 + (1 - Y) * max((0, margin - Dw))^2 / 2 #其中，Dw为模型输出之间的欧氏距离，Y为标签，0或1，margin为边界值 loss = (label * tf.square(d_eu) + (1.0 - label) * tf.square(tf.maximum(self.margin - d_eu, 0.0))) / 2 #返回的loss会被自动reduce_mean return loss 正确率的计算： 损失函数中使用欧氏距离来刻画两个向量之间的相似程度，欧氏距离的值域范围是[0,+∞]，并不适合设置一个阀值来衡量相似或者不相似。解决方法是将欧氏距离映射到[0, 1]的区间（归一化），和余弦相似度的值域一样，接近1就越相似，接近0就越不相似。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def euclidean_distance(x1, x2, keepdims=False): \u0026#39;\u0026#39;\u0026#39; 计算两个tensor的欧氏距离 \u0026#39;\u0026#39;\u0026#39; return tf.sqrt(tf.reduce_sum(tf.square(x1 - x2), axis=1,keepdims=keepdims)) def euclidean_similarity(x1, x2): \u0026#39;\u0026#39;\u0026#39; 将两个tensor之间的欧氏距离转化为相似度，主要是一个归一化的操作 \u0026#39;\u0026#39;\u0026#39; d = euclidean_distance(x1, x2) s = 1.0 / (1.0 + d) return s 在计算正确率的时候，将测试样本x1，支持集样本x2（n个类就有n个x2）依次通过孪生网络，得到n个一维相似度数组，最后做argmax运算即可得出x1是属于n个类中的第几个类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 with tf.device(\u0026#39;/gpu:0\u0026#39;): for x,y in test_iterator: class_probability = [] y1 = model.call_once(x) for i in range(support_n): x2 = support_vector[i] #对y2进行batch_size复制 x2 = np.expand_dims(x2,axis=0) x2 = a= np.repeat(x2, x.shape[0], axis=0) y2 = model.call_once(x2) #计算相似度 s = euclidean_similarity(y1,y2) #shape: (batch_size,) class_probability.append(s) # print(np.array(class_probability).shape) y_pred = tf.argmax(class_probability, axis=0).numpy().astype(np.int32) test_acc = np.sum(y_pred == y) / y.shape[0] 训练： 机器配置：CPU：i5-8300H；显卡：1050TI；内存：16G；显存：4G\n1 2 3 BATCH_SIZE = 32 EPOCH = 2 LEARNING_RATE = 0.001 使用Adam优化器。 训练时，让输入x1和x2分别通过模型，得到两个输出y1和y2，再用y1和y2计算损失和正确率。 训练损失： 训练正确率： 测试： 每个类的训练样本只有50个，训练时候正确率已经接近100%（过拟合问题后面讨论）。但是测试集的正确率训练的过程中的正确率相差甚远。 （支持集的作用是当一个类示例，看测试样本和示例样本是否相似） 将训练数据（14个类 x 每个类50条样本）求和作为支持集： 平均正确率只有7%，出现了某一批次的测试数据正确率达100%，其余测试数据正确率为0，原因未知。 原因分析： 本实验每个类别使用50个样本，满足小样本的特性，但是模型训练完成之后，用测试集测试模型，正确率不到40%。孪生网络应用在人脸识别模型较多（即是输入两张人脸，看这两张人脸是不是属于同一个人），训练一个模型只需要少量样本。人脸图像的特征较为明显（有鼻子、有眼睛等），图像特征也相对容易捕捉。但是当将孪生网络用于少样本的文本分类任务后，一个类别的文本表述方式千变万化，很难通过少量样本找到该类文本的明显特征。 比如，对于财经类的文本：“铜价上涨趋势不变 短期震荡后进一步上扬”出现在了训练集当中，假设这个样本也是财经类的支持集样本，当测试集遇到“午评：期市全线反弹 有色金属郑糖领涨”，出现了“铜”、“有色金属”的特征，则孪生网络可以认为属于同一个类别；但是当测试集样本“保命比业绩更重要 基金上演熊市年底冲刺”，当“基金”、“熊市”等特征没有出现在训练集时，孪生网络则不能正确地划分该类。\n综上所述，孪生网络用于多类别文本分类不可行。\n","date":"2020-03-05T00:00:00Z","permalink":"https://charent.github.io/p/%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BBsiamese-network/","title":"孪生网络实现多分类（Siamese Network）"},{"content":"数据集： 新闻标题数据集，14个类，训练集共75万条数据。\n解决思路： 使用“一对多”拆分策略：每次将一个类别作为正类（正样本），其余其他类别作为反类（负样本），产生N个二分类任务。 为什么不用“一对一”的拆分策略？一对一的拆分策略会产生 N ( N - 1 ) / 2 个分类任务，100个分类将会产生约5000个分类器，训练5000个分类器会花费大量时间，而“一对一”拆分将训练100个分类器，花费的时间更少。\n“一对多”策略产生的问题： 正负样本不均衡。当训练一个类别的时候，其余类别都是反类，产生了正负样本不均衡的问题。 解决方法： 每一类的训练数据中，正样本选N个，负样本从其余类别的样本中随机选N个样本。分批、分类别训练，随机取负样本，负样本尽可能多地包含所有其他类别。\n字向量： 使用开源的基于人民日报语料、Word + Ngram训练的预训练词向量，包含1664K的字、词。每个字、词的维度是300。向量矩阵大小：972M\n数据预处理： 数据集共包含14个类别，每个类别的样本从几万到二十几万不等，为了避免加载二十几万的正样本+二十几万的负样本造成内存爆炸，故限制每个类别的样本不超过5万条。 经过处理后，14个类的训练数据共 42 6208 条，测试数据 3 2081 条 统计句子长度，如下图。取句子最大长度为30，大于30的句子做截断处理，小于30的用0填充。 模型设计： 1 2 3 4 5 6 7 BinaryClassifyModel( Conv1D(filters=196, kernel_size=4, strides=2, activation=\u0026#39;relu\u0026#39;) Conv1D(filters=64, kernel_size=3, strides=2, activation=\u0026#39;relu\u0026#39;) Dense(128, activation=\u0026#39;relu\u0026#39;) Dense(64, activation=\u0026#39;relu\u0026#39;) Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ) 损失函数：二分类交叉熵 2层1维卷积+3层全连接，模型的最后输出的激活函数为sigmoid，sigmoid函数将输出映射到[0,1]，当sigmoid输出 $y\u0026gt;0.5$ 时，认为输入x属于当前分类器的类；当sigmoid输出 $y ≤ 0.5$ 时，认为输入x不属于该类别。\n训练： 机器配置：CPU：i5-8300H；显卡：1050TI；内存：16G；显存：4G\n1 2 3 BATCH_SIZE = 32 #批次大小 EPOCH = 2 #训练轮次 LEARNING_RATE = 0.005 #学习率 使用Adam优化器 每个类训练完成后，将模型保存到 ./model_save/class_ + category_id 文件夹内，category_id 为分类id，从0到13，共14个分类。 42万训练数据，14个二分类器训练共用了15分49秒，平均每个分类器训练用时67秒。 训练过程中的平均正确率没有统计，下图展示类别6的在训练过程中的损失和正确率： 测试： 模型训练完成后会将模型的参数保存到文件中，测试时先从文件加载训练好的二分类模型，再让测试数据依次通过14二分类模型，记录下每个二分类模型的输出概率（由于使用了sigmoid函数，输出在[0,1]之间），哪个二分类模型的输出概率最大，则认为测试样本属于该类。 测试数据如下图所示，平均正确率：88.59%，3万条测试数据共花了40秒，平均每个样本通过14个二分器用时1.3毫秒。 结论： 不管从训练用时、模型计算用时、模型大小等方面考虑，使用二分类实现多分类对文本分类可行。新增加一个分类，只需要新添加一个二分类器即可，当分类更改时对二分类器做微调也比较方便，不改变模型的输入输出大小。不像传统做法：只有一个模型，模型参数巨大，新增或者删除一个分类，要对模型最后的全连接层调整，以调整输出的大小。对于多标签分类，每个标签训练一个二分类器即可解决。对于长文本，可以用自注意力层替换第一层卷积层，最大文本长度可以支持1000以上。\n缺点：需要大量的训练数据。 附代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 import tensorflow as tf import numpy as np from matplotlib import pyplot as plt from gensim.models import KeyedVectors import datetime TRAIN_TEXT = \u0026#39;./data/新闻标题分类Train.txt\u0026#39; TEST_TEXT = \u0026#39;./data/新闻标题分类Test.txt\u0026#39; #人民日报预训练字向量 EMBEDDINGD_DIR = \u0026#39;../Word_Embeddings/sgns.renmin.bigram-char\u0026#39; embedding = KeyedVectors.load_word2vec_format(EMBEDDINGD_DIR, binary=False,limit=50000) def word_to_vector(words_array, words_embedding, max_len=30): \u0026#39;\u0026#39;\u0026#39; 将字转为字嵌入，words_array为一维数组，每个元素是一句话 \u0026#39;\u0026#39;\u0026#39; words_vector = [] i = 0 for sentence in words_array: if max_len != None: current_vector = np.zeros((max_len,300),dtype = np.float32) else: current_vector = np.zeros((len(sentence), 300), dtype = np.float32) # print(current_vector.shape) index = 0 for word in sentence: try: current_vector[index] = words_embedding[word] index += 1 if index == max_len: break except: #可能会出现某个字在embedding中不存在的情况，则跳过这个字 continue #end for words_vector.append(current_vector) # print(words_vector.shape) # break #end for return np.array(words_vector) def read_data(train_text): \u0026#39;\u0026#39;\u0026#39; 读取训练文件，并划分训练集和测试集 \u0026#39;\u0026#39;\u0026#39; category = [] news_data = [] news_label = [] with open(train_text,\u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: lines = f.readlines() current_category = int(\u0026#39;0\u0026#39;) category.append([current_category, \u0026#39;财经\u0026#39;]) #[\u0026#39;0\u0026#39;, \u0026#39;财经\u0026#39;, \u0026#39;上证50ETF净申购突增\\n\u0026#39;] #每个类别保留的最大样本数目，防止内存爆炸 count = 0 max_keep = 50000 for line in lines: line = line.strip().split(\u0026#39;\\t\u0026#39;) cate = int(line[0]) #还是当前类，但是count已经到达最大max_keep，不处理 if cate == current_category and count \u0026gt;= max_keep: continue if cate != current_category: # print(count) current_category = cate category.append([current_category, line[1]]) count = 0 news_data.append(line[2]) news_label.append(cate) count += 1 news_data = np.array(news_data) news_label = np.array(news_label) #划分训练集和测试集 n = len(news_data) sample_index = np.random.permutation(n).astype(np.int32) train_index = sample_index[ : int(n * 0.93)] test_index = sample_index[int(n * 0.93) : ] train_data = news_data[train_index] train_label = news_label[train_index] test_data = news_data[test_index] test_label = news_label[test_index] return np.array(category), train_data, train_label, test_data,test_label #取出第一类作为正样本,再从其余类中取出负样本 def get_category_sample(category_id, data, label, batch_size=None): \u0026#39;\u0026#39;\u0026#39; 取出一个类作为正样本，再从其余类中随机选出和正样本数量相当的负样本，尽可能包含所有类 \u0026#39;\u0026#39;\u0026#39; pos_sample_index = [] other_sample_index = [] n = len(label) for index in range(n): if category_id == label[index]: pos_sample_index.append(index) else: other_sample_index.append(index) #正样本个数 pos_n = len(pos_sample_index) #随机取跟正样本数量一样的负样本 neg_sample_index = np.random.choice(other_sample_index, size=pos_n, replace=False) #将正负样本的index拼接并打乱顺序 sample_index = np.concatenate((pos_sample_index, neg_sample_index)).astype(np.int32) np.random.shuffle(sample_index) #用sample_index取出样本 n = len(sample_index) train_index = sample_index[ : int(n * 0.9)] test_index = sample_index[int(n * 0.9) : ] train_data = data[train_index] train_label = label[train_index] # print(train_index.shape) test_data = data[test_index] test_label = label[test_index] #将当前类的label设置为1，其他类设置为0 def change_label(lab): n = len(lab) for i in range(n): if lab[i] == category_id: lab[i] = 1 else: lab[i] = 0 return lab train_label = change_label(train_label) test_label = change_label(test_label) return train_data, train_label, test_data, test_label #统计标题长度 def count_len(data): max_len = 0 n = len(data) len_dict = {} for i in range(n): sentence_len = len(data[i]) if sentence_len in len_dict: len_dict[sentence_len] += 1 else: len_dict[sentence_len] = 1 lengths = [key for key in len_dict.keys()] lengths.sort() count = [len_dict[key] for key in lengths] plt.bar(lengths, count) plt.show() #二分类模型 class BinaryClassifyModel(tf.keras.Model): def __init__(self,filters=196, kernel_size=4): super().__init__() self.conv1 = tf.keras.layers.Conv1D(filters, kernel_size, strides=2, activation=\u0026#39;relu\u0026#39;) self.maxpool1 = tf.keras.layers.MaxPool1D(strides=2) self.conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation=\u0026#39;relu\u0026#39;) self.maxpool2 = tf.keras.layers.MaxPool1D(strides=2) self.fc1 = tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;) self.fc2 = tf.keras.layers.Dense(64, activation=\u0026#39;relu\u0026#39;) self.fc3 = tf.keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) def call(self, inputs): #卷积层 x = self.conv1(inputs) #shape: batch_size,new_steps(stpe / strides (- 1)), filters ,(batch_size, 14 ,196) x = self.maxpool1(x) #(batch_size,7, 196) x = self.conv2(x) #(batch_size,5, 64) x = self.maxpool2(x) #(batch_size,2, 64) #将三维向量reshape为二维矩阵，用于全连接层输入 x = tf.reshape(x, (tf.shape(x)[0], -1)) #(batch_size, 128) # print(x.shape) #全连接层 x = self.fc1(x) x = self.fc2(x) # output = tf.nn.softmax(self.fc3(x)) output = self.fc3(x) return tf.reshape(output, [output.shape[0]]) def train_a_classifier(train_vector,train_label, model, save_dir,learning_rate=0.001, msg=\u0026#39;\u0026#39;): \u0026#39;\u0026#39;\u0026#39; 训练一个模型， \u0026#39;\u0026#39;\u0026#39; train_iterator = tf.data.Dataset.from_tensor_slices((train_vector, train_label)).batch(BATCH_SIZE).repeat(EPOCH) optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) check_point = tf.train.Checkpoint(binary_model = model) checkpoint_manager = tf.train.CheckpointManager(check_point, directory=save_dir, max_to_keep=1) acc_ary = [] loss_ary = [] steps = [] with tf.device(\u0026#39;/gpu:0\u0026#39;): for step,(x, y) in enumerate(train_iterator): with tf.GradientTape() as tape: y_pred = model(x) #损失函数,二分类交叉熵 loss = tf.keras.losses.binary_crossentropy(y_pred=y_pred, y_true=y) loss = tf.reduce_mean(loss) #计算正确率 y_pred = tf.where(y_pred \u0026lt; 0.5, x=tf.zeros_like(y_pred), y=tf.ones_like(y_pred)) y_pred = y_pred.numpy().astype(np.int32) train_acc = np.sum(y_pred == y) / y.shape[0] # print(y_pred) # print(y) # print(train_acc) # break #统计损失和正确率，用于画图 loss_ary.append(loss.numpy()) acc_ary.append(train_acc) steps.append(step) grads = tape.gradient(loss, model.variables) optimizer.apply_gradients(grads_and_vars= zip(grads, model.variables)) if step % 200 == 0: print(\u0026#34;{} setp: {} , loss : {} , train acc : {}\u0026#34;.format(msg, step, loss, train_acc)) plt.plot(steps, loss_ary, c=\u0026#39;r\u0026#39;) plt.title(msg + \u0026#39;loss\u0026#39;) plt.show() plt.plot(steps, acc_ary, c=\u0026#39;b\u0026#39;) plt.title(msg + \u0026#39;accuracy\u0026#39;) plt.show() path = checkpoint_manager.save() print(\u0026#39;保存模型到：{}\u0026#39;.format(path)) return model def train_all_classifier(category, train_data, train_label): \u0026#39;\u0026#39;\u0026#39; 训练所有分类器 categroy：二维数组，第一列为category_id, 第列为id对应的名称 train_data,train_laber：所有类别的训练数据和标签，都是一维数组，一个元素为一句话或一个标签 \u0026#39;\u0026#39;\u0026#39; models_list = [] models_save_dir = [] # print(train_data.shape) for cate in category: model = BinaryClassifyModel() category_id = int(cate[0]) print(\u0026#39;当前训练：{}\u0026#39;.format(category_id)) start_time = datetime.datetime.now() data_train, label_train, data_test, label_test = get_category_sample(category_id, train_data, train_label) train_vector = word_to_vector(data_train, embedding, max_len=30) print(\u0026#39;准备数据用时：{}\u0026#39;.format(datetime.datetime.now() - start_time)) # test_vector = word_to_vector(data_test, embedding, max_len=30) #MODEL_SAVE_DIR命名规则，MODEL_SAVE_DIR + category_id，如./model_save/class_1，./model_save/class_2 model = train_a_classifier(train_vector, label_train, model,save_dir=MODEL_SAVE_DIR + str(category_id), learning_rate=LEARNING_RATE, msg=\u0026#39;class_\u0026#39;+str(category_id)) print(\u0026#39;训练用时：{}\u0026#39;.format(datetime.datetime.now() - start_time)) models_list.append(model) def test_all_classifier(category, test_data, test_laber, need_load_checkpoint=False, model_ary=None): \u0026#39;\u0026#39;\u0026#39; 测试所有分类器的性能 \u0026#39;\u0026#39;\u0026#39; start_time = datetime.datetime.now() test_vector = word_to_vector(test_data, embedding, max_len=30) print(\u0026#39;准备数据用时：{}\u0026#39;.format(datetime.datetime.now() - start_time)) #从文件中加载训练好的模型 if need_load_checkpoint: model_ary = [] for cate in category: category_id = int(cate[0]) model = BinaryClassifyModel() check_point = tf.train.Checkpoint(binary_model = model) check_point.restore(tf.train.latest_checkpoint(MODEL_SAVE_DIR + str(category_id))) model_ary.append(model) start_time = datetime.datetime.now() test_iterator = tf.data.Dataset.from_tensor_slices((test_vector, test_label)).batch(BATCH_SIZE) acc_ary = [] count = 0 with tf.device(\u0026#39;/gpu:0\u0026#39;): for x,y in test_iterator: class_probability = [] n = len(model_ary) #测试样本依次通过每个二分类分类器，并记录下是该类的概率,哪个分类器的输出概率最大，则认为是哪个类 for i in range(n): y_pro = model_ary[i](x) class_probability.append(y_pro) y_pred = tf.argmax(class_probability, axis=0).numpy().astype(np.int32) test_acc = np.sum(y_pred == y) / y.shape[0] acc_ary.append(test_acc) count += y.shape[0] #endfor #end with end_time = datetime.datetime.now() print(\u0026#39;平均正确率: {}\u0026#39;.format(np.average(acc_ary))) print(\u0026#39;测试总用时：{}\u0026#39;.format(end_time - start_time)) print(\u0026#39;共测试 {} 个样本，平均每个样本计算耗时：{}\u0026#39;.format(count, (end_time - start_time) / count)) plt.plot(np.arange(0, len(acc_ary)), acc_ary, c=\u0026#39;g\u0026#39;) plt.title(\u0026#39;test accuracy\u0026#39;) plt.show() BATCH_SIZE = 32 EPOCH = 2 LEARNING_RATE = 0.005 LOG_DIR = \u0026#39;./model_log\u0026#39; MODEL_SAVE_DIR = \u0026#39;./model_save/class_\u0026#39; #读取训练数据 category, train_data, train_label, test_data,test_label = read_data(TRAIN_TEXT) #统计句子长度，确定最长序列 count_len(train_data) #训练所有分类器 start_time = datetime.datetime.now() models_list = train_all_classifier(category, train_data, train_label) end_time = datetime.datetime.now() print(\u0026#39;训练总用时：{}\u0026#39;.format(end_time - start_time)) #测试所有分类器 test_all_classifier(category, test_data, test_label,need_load_checkpoint=True) ","date":"2020-02-21T00:00:00Z","permalink":"https://charent.github.io/p/%E4%BD%BF%E7%94%A8%E4%BA%8C%E5%88%86%E7%B1%BB%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BB/","title":"使用二分类实现多分类"},{"content":"多标签分类问题 有多个类别，但每个样例可能对应多个类别，因此这些问题被称为多类分类问题。 通过一份体检报告判断一个人是否患有以下五种病：有序排列——[高血压，高血糖，肥胖，肺结核，冠心病]，一个样本[1,0,1,0,0] ，其中1代表该位置的患病，0代表没患病。所以这个label的含义：患者有高血压和肥胖。\n解决多标签分类问题的方法： 基本上，有三种方法来解决一个多标签分类问题，即:\n问题转换 改编算法 集成方法 问题转换： 在这个方法中，我们将尝试把多标签问题转换为单标签问题。这种方法可以用三种不同的方式进行: 二元关联（Binary Relevance） 分类器链（Classifier Chains） 标签Powerset（Label Powerset）\n二元关联（Binary Relevance） 这是最简单的技术，它基本上把每个标签当作单独的一个类分类问题。例如，让我们考虑如下所示的一个案例。我们有这样的数据集，X是独立的特征，Y是目标变量。 在二元关联中，这个问题被分解成4个不同的类分类问题，如下图所示。 分类器链（Classifier Chains） 在这种情况下，第一个分类器只在输入数据上进行训练，然后每个分类器都在输入空间和链上的所有之前的分类器上进行训练。 让我们试着通过一个例子来理解这个问题。在下面给出的数据集里，我们将X作为输入空间，而Y作为标签。 在分类器链中，这个问题将被转换成4个不同的标签问题，就像下面所示。黄色部分是输入空间，白色部分代表目标变量。这与二元关联非常相似，唯一的区别在于它是为了保持标签相关性而形成的。 标签Powerset（Label Powerset） 在这方面，我们将问题转化为一个多类问题，一个多类分类器在训练数据中发现的所有唯一的标签组合上被训练。让我们通过一个例子来理解它。 在这一点上，我们发现x1和x4有相同的标签。同样的，x3和x6有相同的标签。因此，标签powerset将这个问题转换为一个单一的多类问题，如下所示。 因此，标签powerset给训练集中的每一个可能的标签组合提供了一个独特的类。\n深度学习方法： 模型输入输出 假设我们有一个体检疾病判断任务：通过一份体检报告判断一个人是否患有以下五种病：有序排列——[高血压，高血糖，肥胖，肺结核，冠心病] 输入：一份体检报告 输出：[1,0,1,0,0 ] ，其中1代表该位置的患病，0代表没患病。所以这个label的含义：患者有高血压和肥胖。\n模型架构 接下来如何建立模型呢: 当然可以对label的每一个维度分别进行建模，训练5个二分类器。 但是这样不仅是的label之间的依赖关系被破坏，而且还耗时耗力。\n接下来我们还是来看看深度神经网络是如何应用于此问题的。其架构如下： 采用神经网络做特征提取器，这部分不需要多解释，就是一个深度学习网络； 采用sigmoid做输出层的激活函数，若做体检疾病判断任务的话输出层是5个节点对应一个5维向量，这里没有采用softmax，就是希望sigmoid对每一个节点的值做一次激活，从而输出每个节点分别是 1 概率；\n采用binary_crossentropy损失函数函数，这样使得模型在训练过程中不断降低output和label之间的交叉熵。其实就相当于模型使label为1的节点的输出值更靠近1，label为0的节点的输出值更靠近0。\n有点类似 Structure Learing ，最终模型的输出就是一个结构序列。\n","date":"2020-02-10T00:00:00Z","permalink":"https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/","title":"多标签分类"},{"content":"从下图(a)可以看出，每次迭代选取的批量样本数越多，下降效果越明显，并且取现越平滑。当每次选取一个样本时（相当于随机梯度下降），损失整体是下降趋势，但局部看会来回震荡。 从(b)可以看出，如果按整个数据集迭代的来看损失变化情况，则小批量样本数越小，下降效果越明显。 ","date":"2020-01-07T00:00:00Z","permalink":"https://charent.github.io/p/%E8%AE%AD%E7%BB%83%E6%97%B6batch_size%E5%A4%A7%E5%B0%8F%E5%AF%B9%E6%94%B6%E6%95%9B%E7%9A%84%E5%BD%B1%E5%93%8D/","title":"训练时batch_size大小对收敛的影响"},{"content":"AB Test： A/B 测试是为Web或App界面或流程制作两个（A/B）或多个（A/B/n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用。\n点击通过率（Click-through Rate，CTR）: 一般指网络广告的点击到达率，即该广告的实际点击次数除以广告的展现量，即clicks/views。反映了网页上某一内容的受关注程度，常常用来衡量广告的吸引程度\nROC曲线 接受者操作特性曲线（receiver operating characteristic curve，简称ROC曲线）。曲线的横坐标为假阳性率（False Positive Rate, FPR）\n1 FPR=\\frac {FP} {(FP+TN)} N是真实负样本的个数， FP是N个负样本中被分类器预测为正样本的个数。 纵坐标为真阳性率（True Positive Rate, TPR）\n1 TPR=\\frac {TP}{P}=\\frac {TP} {(TP+FN)} 其中，P是真实正样本的个数，TP是P个正样本中被分类器预测为正样本的个数。\nAUC （ROC曲线下方的面积大小）： AUC（Area Under Curve）被定义为ROC曲线下与坐标轴围成的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。AUC越接近1.0，检测方法真实性越高;等于0.5时，则真实性最低，无应用价值。我们往往使用AUC值作为模型的评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好\n","date":"2019-12-26T00:00:00Z","permalink":"https://charent.github.io/p/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%A1%A1%E9%87%8F%E6%96%B9%E6%B3%95/","title":"推荐系统衡量方法"},{"content":"负采样 自然语言处理领域中，判断两个单词是不是一对上下文词（context）与目标词（target），如果是一对，则是正样本，如果不是一对，则是负样本。 采样得到一个上下文词和一个目标词，生成一个正样本（positive example），生成一个负样本（negative example），则是用与正样本相同的上下文词，再在字典中随机选择一个单词，这就是负采样（negative sampling）。\n负采样是为了解决类别太多的一种折中方案，样本只是给模型训练提供信息的，那负样本的选择肯定是选信息量大的那些，比如一些模型的决策边界，如果有充足的样本就能学的比较好，如果负样本离分离边界太远，那其实提供不了太多有用信息，甚至会误导模型使其有偏。\n还有就是一些任务比如预测任务，负采样可能会使其偏度较大，比如点击率预估，本来样本点击率为0.01，负采样使正负样本比例1:9，那最后样本平均点击率就为0.1，这种任务如果一定要负采样肯定要进行一定的修正。 如果是一般的任务，其实负样本选择对效果的影响很大。主要看数据分布，分布波动较大，样本噪声高的任务，负采样很难。 其原理就是正常正负样本对参数影响的原理。而且一般都不是随机负采样，都是按照一定权重方法采样，进而也揭示了负采样其实不能随便采。\nnegative sampling在以下几种情况可能会有不一样的结果。\n样本信息过分冗余，通过negative sampling可以在相同机器资源的情况下提高训练速度，而且对效果影响很有限，这对于有限预算下是很重要的。 负样本不能有效反应用户真实意图的情况下，negative sampling可能会带来收益，比如有一些场景用户很可能大部分都没有看到而导致的负样本采集； 对于不同的问题也可能会不太一样，比如说implicit和explicit的问题，implict的feedback本身也是有折损的，也就是不点击不代表不喜欢，点击也不代表一定喜欢，需要考虑的信号就需要更仔细的看了。 ","date":"2019-12-19T00:00:00Z","permalink":"https://charent.github.io/p/%E8%B4%9F%E6%A0%B7%E6%9C%AC%E8%B4%9F%E9%87%87%E6%A0%B7/","title":"负样本（负采样）"},{"content":"什么是TF-IDF？ TFIDF全程叫做term frequency–inverse document frequency，翻译过来可以叫做文本频率与逆文档频率指数，TFIDF就是为了表征一个token（可以是一个字或者一个词）的重要程度，所以如果这个token出现的频数很高，会更重要一点\n在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。\n文本向量化特征的不足 在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，我们将下面4个短文本做了词频统计：\n1 2 3 4 corpus=[\u0026#34;I come to China to travel\u0026#34;, \u0026#34;This is a car polupar in China\u0026#34;, \u0026#34;I love tea and Apple \u0026#34;, \u0026#34;The work is to write some papers in science\u0026#34;] 不考虑停用词，处理后得到的词向量如下：\n1 2 3 4 [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]] 如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现\u0026quot;come\u0026quot;,\u0026ldquo;China\u0026quot;和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的\u0026quot;China\u0026quot;和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。\n前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。\n概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。 上面是从定性上说明的IDF的作用，那么如何对一个词的IDF进行定量分析呢？这里直接给出一个词x的IDF的基本公式如下： $$ IDF(x)=log(\\frac{N+1}{N(x)+1}) +1 $$ 其中，N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数。为什么IDF的基本公式应该是是上面这样的而不是像N/N(x)这样的形式呢？这就涉及到信息论相关的一些知识了。 上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为\n有了IDF的定义，我们就可以计算某一个词的TF-IDF值了： $$ TFIDF(x)=TF(x)∗IDF(x) $$ 其中TF(x)指词x在当前文本中的词频。\nTF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。 当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。\n","date":"2019-12-17T00:00:00Z","permalink":"https://charent.github.io/p/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8Btfidf/","title":"文本预处理之TFIDF"},{"content":"为什么要正则化？ 深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。 如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差。 Dropout： Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？\n​直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。\nDropout缺点： dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。\n","date":"2019-12-05T00:00:00Z","permalink":"https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/","title":"深度神经网络的正则化"},{"content":"批量归一化（Batch Normalization） 训练深度神经网络可能非常耗时。但是可以通过消除梯度来显着地减少训练时间，这种情况发生在网络由于梯度（特别是在较早的层中的梯度）接近零值而停止更新。\n在CNN中，BN应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度\n批量标准化（Batch Normalization）也许是对付梯度消失和爆炸问题的最有力工具。 批量标准化的工作方式如下：对于给定层中的每个单元，首先计算z分数，然后在两个受过训练的变量γ和β应用线性转换。 批量标准化通常在非线性激活函数之前完成，但在激活函数之后应用批量标准也可能是有利的。 σ激活函数和tanh激活函数存在梯度饱和的区域，其原因是激活函数的输入值过大或者过小，其得到的激活函数的梯度值会非常接近于0，使得网络的收敛速度减慢。传统的方法是使用不存在梯度饱和区域的激活函数，例如ReLU等。BN也可以缓解梯度饱和的问题，它的策略是在调用激活函数之前将Wx+b的值归一化到梯度值比较大的区域。BN应在激活函数之前使用。\nσ激活函数和tanh激活函数： 在反向传播过程中，梯度倾向于在较低层里变得更小，从而减缓权重更新并因此减少训练次数。 批量标准化有助于消除所谓的梯度消失问题。\n在批量归一化的论文中 https://arxiv.org/abs/1502.03167v3，作者是娘BN放在非线性激活层的前面。目前在实践上，倾向于把BN放在ReLU后面。也有评测表明BN放ReLU后面效果更好。\u0026hellip;（-）\u0026hellip;玄学\n","date":"2019-12-04T00:00:00Z","permalink":"https://charent.github.io/p/batch-normalization%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/","title":"Batch Normalization：批量归一化"},{"content":" 1 2 3 4 #对于一些简单的for循环用map和lambda优化，加快脚本运行速度， 如 lines = list(map(lambda x : list(eval(x)), lines)) #对于一些经常要判定一个元素是否在某个数组里面的操作，将数组转换为集合，加快查找速度 set(map(lambda x : x[0], n_movies)) 数据处理部分 电影间的依赖关系：使用networkx随机生成有向无环图，共生成10个有向无环图，根据生成的图可以生成业务矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 矩阵的列：vertex, next_job, negative_job, clicked_job, parents 一个顶点的next_job生成： 1)取该顶点v的孩子顶点，记为v_children 2)取该顶点v所有父顶点中孩子顶点，但这些孩子顶点不是v的父顶点,记为v_p_c 3)v_children和v_p_c取并集，该并集即是next_job 一个顶点的negative_job生成： 1)取该顶点v所在连通子图的所有顶点v_connect 2)在v_connect中删除该顶点v本身，删除v的所有孩子顶点，删除v的所有父顶点以及父顶点的孩子顶点，剩余的顶点记为v_res 3)将v_res和其它连通子图（即所有非顶点v所属的连通子图）的所有顶点取并集，该并集即是negative_job clicked_job=parents 数据处理过程 电影选取：统计数据集中每部电影被用户交互过的次数，从大到小排序，从交互比较稠密的前200个电影中随机抽取152部。 根据选取出的152部电影选出交互过这些电影是用户，如果用户交互过的所有电影的负样本的交集为空，则不选择该用户；并且，每个用户交互过的电影必须大于4个，防止用户交互信息过于稀疏。 生成用户行为数据：根据选出的用户和其交互过的电影信息，生成[用户id，业务id，评分，上一次交互id，时间戳] 格式的行为数据，上一层交互id不存在用-1代替，业务id即是152个电影id。 将选出的电影特征做处理，将电影上映年份、种类处理为one hot向量。 将用户信息的邮政编码归一化处理，将用户的性别、职业、年龄等信息处理为 one hot 向量。 将电影标题使用bert_as_service生成1x768的句子向量，给定一个在一定单词数在一定范围内的句子，bert as service能生成该句子的1x768的向量。 编写DataInput类，用于将行为数据中的用户id和用户信息（职业性别等向量）、电影id和电影信息（电影类型、发行年份的one hot向量，bert句子向量等）关联起来，生成一条样本数据。编写next_batc()方法用于生产下一批次的样本数据，作为模型的输入，next_batc()函数用参数isTrain来判断当前的next batch是训练数据还是测试数据。 模型部分 模型如图所示 在忽略batch size的情况下，用户信息的输入tensor为 1x30；正样本的输入tensor为 2 * 796，2是lstm cell输入的max time，即是有2个时间序列，表示电影依赖关系a-\u0026gt;b的序列，796中的768是电影标题的句子向量，剩下的28是电影种类、发行年份的one hot 向量；负样本的输入仅仅是一个电影的信息向量，1x796。\n封装tensorflow的全连接层函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def add_dense(input_tensor, output_size, layer_name, keep_prob=1.0, activation_function=None, reuse=False): \u0026#39;\u0026#39;\u0026#39;添加一个全连接层。 输入的tensor，输出的维度，层的名字 \u0026#39;\u0026#39;\u0026#39; #获得tensor的列数（shape的第二个维度） input_cloumn = input_tensor.get_shape()[1] #计算dropout_rate # dropout_rate = 1.0 - keep_prob with tf.variable_scope(\u0026#39;dense_layer_\u0026#39; + layer_name, reuse=reuse): weights = tf.get_variable(\u0026#39;weights\u0026#39;, [input_cloumn, output_size], initializer=tf.truncated_normal_initializer(stddev=0.1)) biases = tf.get_variable(\u0026#39;biases\u0026#39;, [output_size], initializer=tf.constant_initializer(0.0001)) variable_summaries(weights, layer_name + \u0026#39;_weights\u0026#39;) variable_summaries(biases, layer_name + \u0026#39;_biases\u0026#39;) mul_op = tf.matmul(input_tensor, weights) + biases output_tensor = None if activation_function == None: output_tensor = mul_op else: output_tensor = activation_function(mul_op) if keep_prob != 1.0: output_tensor = tf.nn.dropout(output_tensor, keep_prob=keep_prob) return output_tensor 封装tensorflow的LSTM层函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def add_LSTM(input_tensor, num_units_per_layer_list, layer_name, keep_prob=1.0, reuse=False): \u0026#39;\u0026#39;\u0026#39;添加一个长短期记忆网络。 input_tensor：输入tensor，[batch_size, max_time, cell.output_size] num_units_per_layer_list：LSTM的隐藏单元个数列表 \u0026#39;\u0026#39;\u0026#39; # drop_rate = 1.0 - keep_prob with tf.variable_scope(\u0026#39;lstm_layer_\u0026#39; + layer_name, reuse=reuse): lstm_layers = [] for size in num_units_per_layer_list: lstm_cell = tf.contrib.rnn.BasicLSTMCell(size) if keep_prob != 1.0: lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob) lstm_layers.append(lstm_cell) if len(lstm_layers) \u0026gt; 1: muti_Lstm_cells = tf.contrib.rnn.MultiRNNCell(lstm_layers) # init_state = muti_Lstm_cells.zero_state(batch_size, dtype=tf.float32) _, final_state = tf.nn.dynamic_rnn(cell=muti_Lstm_cells, inputs=input_tensor, dtype=tf.float32) # \u0026#39;outputs\u0026#39; is a tensor of shape [batch_size, max_time, sequence_length] # \u0026#39;state\u0026#39; is a N-tuple where N is the number of LSTMCells containing a tf.nn.rnn_cell.LSTMStateTuple for each cell output = final_state[-1][1] else: _, final_state = tf.nn.dynamic_rnn(lstm_layers[0], input_tensor, dtype=tf.float32) output = final_state[1] return output 计算余弦相似度：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 with tf.name_scope(\u0026#39;calu_cos_similarity\u0026#39;): #user_item和postive_item的相似度 #||user_norm|| = sqrt(sum(each ^ 2)) user_norm = tf.sqrt(tf.reduce_sum(tf.square(user_dense_output), axis=1, keep_dims=True)) #||pos_norm|| = sqrt(sum(each ^ 2)) pos_norm = tf.sqrt(tf.reduce_sum(tf.square(pos_dense_output), axis=1, keep_dims=True)) # user_dense_output * pos_dense_output user_pos = tf.reduce_sum(tf.multiply(user_dense_output, pos_dense_output), axis=1, keep_dims=True) #||user_norm|| * ||pos_norm||，作为相似度的分母 user_pos_norm_prod = tf.multiply(user_norm, pos_norm) #用户item输出和正样本输出的相似度R(user, pos)，R_user_pos = (user_dense_output * pos_dense_output) / ||user_norm|| * ||pos_norm|| R_user_pos = tf.truediv(user_pos, user_pos_norm_prod) #||neg_norm|| = sqrt(sum(each ^ 2)) neg_norm = tf.sqrt(tf.reduce_sum(tf.square(neg_dense_output), axis=1, keep_dims=True)) #user_neg = user_dense_output * neg_dense_output user_neg = tf.reduce_sum(tf.multiply(user_dense_output, neg_dense_output), axis=1, keep_dims=True) #||user_norm|| * ||neg_norm|| user_neg_norm_prod = tf.multiply(user_norm, neg_norm) #用户item输出和负样本输出的相似度R(user, neg)，R_user_neg = (user_dense_output * neg_dense_output) / ||user_norm|| * ||neg_norm|| R_user_neg = tf.truediv(user_neg, user_neg_norm_prod) 计算后验概率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 with tf.name_scope(\u0026#39;calc_posterior_probability\u0026#39;): #exp(gamma * R(User, item)) GAMMA = 1.0 exp_R_user_pos = tf.exp(tf.multiply(GAMMA, R_user_pos)) exp_R_user_neg = tf.exp(tf.multiply(GAMMA, R_user_neg)) #sum_exp_R = exp(gamma * R_user_pos) + exp(gamma * R_user_neg) sum_exp_R = tf.add(exp_R_user_pos, exp_R_user_neg) #p(pos | user) = exp_R_user_pos / sum_exp_R prob_pos_user = tf.truediv(exp_R_user_pos, sum_exp_R) #p(neg | user) = exp_R_user_neg / sum_exp_R # prob_neg_user = tf.truediv(exp_R_user_neg, sum_exp_R) 损失函数\n1 2 3 4 5 6 7 8 9 with tf.name_scope(\u0026#39;loss_function\u0026#39;): # loss = -log(p(item_1 | user) * ... * p(item_n | user)),约束条件:item_i 属于被激活的item log_prob_pos_user = tf.log(tf.clip_by_value(prob_pos_user, 1e-6, 1.0)) # log_prob_neg_user = tf.log(tf.clip_by_value(prob_neg_user, 1e-6, 1.0)) # loss = -tf.reduce_sum(tf.add(log_prob_pos_user, log_prob_neg_user)) loss = -tf.reduce_sum(log_prob_pos_user) # tf.summary.scalar(\u0026#39;loss\u0026#39;, loss) 训练模型：训练得到的loss要除以batch size，以得到每个输入样本的loss。\n关于tensorflow中LSTM输出：返回的final_states是个二元组(LSTMCell的参数state_is_tuple=True时才返回二元组，默认为true)，state[0]是cell state，final_states[1]是hidden state，可以取hidden state作为lstm的输出（即output = final_states[1]），对于多层lstm网络，取最后一个时间序列的final_state作为输出，即是output = final_states[-1][1]。返回的outputs是记录的每一次的时间序列的输出，对于某些问题（如翻译），需要得到每次时间序列的lstm输出。对于LSTM后还接一个全连接层的模型，可以取所有时间序列的output作为全连接层的输入，output = tf.reshape(outputs, [-1,lstm_unit])，lstm_unit为LSTM的隐藏神经单元个数。一般地，batch size = 1时，final_states.shape = (1, lstm_unit)，outputs.shape = (1, max_time, lstm_unit)，final_states[1][0] = outputs[0][-1]，即是final_state记录了每一条样本输出outputs的最后一次时间序列的输出。 计算召回率：模型训练好之后，使用测试数据集计算召回率。 a. 每次取一个用户（该用户必须在训练集和测试都有行为数据），给定一个估计集中最少推荐电影个数m。 b. 根据用户在训练集中交互过的电影中找出候选顶点，候选顶点：对用户在训练集中交互过的所有电影，找出每个电影顶点的孩子顶点（仅获取一层）、兄弟顶点和所有父顶点的孩子顶点（不包括父顶点本身），最后取并集，得到候选顶点，如若候选顶点不足m个，则从所有顶点中随机取若干个顶点补足m个，这m个电影顶点作为模型的正样本输入。 c. 根据训练好的模型计算用户信息和正样本的cos相似度SP，根据相似度SP从大到小排序，取前top k个顶点集，这k个顶点集和该用户的测试集中电影顶点集test_set取交集，记交集为i_set. d. 该用户的召回率 = len(i_set) / len(test_set)。最后求所有用户的平均召回率（图中仅仅展示求得的召回率，并没有算出评价召回率）。\n模型改进：损失函数加入约束项，防止用户信息与正样本的相似度、用户信息与负样本的相似度过快收敛。 ","date":"2019-07-31T00:00:00Z","permalink":"https://charent.github.io/p/%E5%AD%98%E5%9C%A8%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E7%9A%84%E6%8E%A8%E8%8D%90/","title":"存在依赖关系的推荐"},{"content":"生成当前目录下所有文件的md5值 1 find ./ -type f -print0 | xargs -0 md5sum \u0026gt; ./md5.txt ","date":"2019-02-17T00:00:00Z","permalink":"https://charent.github.io/p/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Linux常用命令"},{"content":"为什么需要激活函数？ 激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。 激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。 激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。 为什么激活函数需要非线性函数？ 假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。 使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。 如何选择激活函数 选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。\n如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。 如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。 sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。 tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。 ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。 如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。 就我个人使用经验而言，非输出层一般使用Relu，复杂网络也会考虑Leaky Relu，输出层：如果是二分类，sigmoid无疑了（当然也可以用softmax），多分类则是softmax。\n激活函数分类 在神经网络计算中，输入X会先进行一个线性变换，\n$$ y = W · X + b $$\n之后再进行一个非线性变换，即是y通过一个非线性的激活函数： $$ output=g(y) $$\n$g(y)$ 为非线性激活函数。\nsigmoid函数\nsgmoid函数的计算公式为：\n$$ g(x)= \\frac{1} {1 + e^{-x} } $$\nsigmoid函数缺点：当 x 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 $g′(x)$ 将接近 0 。这会导致权重 W 的梯度将接近 0 ，使得梯度更新十分缓慢，即梯度消失。\ntanh函数\ntanh函数的计算公式为: $$ g(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\ntanh函数的缺点同sigmoid函数的第一个缺点一样，当 x 很大或很小时，$g′(x)$ 接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。 ReLU函数\nReLU函数计算公式为：\n$$ g(x) = \\left{ \\begin{array}{rcl} x \u0026amp;\u0026amp; {x \u0026gt; 0} \\ 0 \u0026amp;\u0026amp; {x \\le 0} \\end{array} \\right. $$ ReLU函数的优点：\n在输入为正数的时候（对于大多数输入 x 空间来说），不存在梯度消失问题。 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。sigmod和tanh要计算指数，计算速度会比较慢。 ReLU函数的缺点：\n当输入为负时，梯度为0，会产生梯度消失问题。 Leaky ReLU函数\nLeaky Relu函数计算公式： $$ g(x) = \\left { \\begin{array}{cl} x \u0026amp;\u0026amp; {x \u0026gt; 0} \\ \\alpha \\times x \u0026amp;\u0026amp; {x \\le 0} \\end{array} \\right. $$ 其中，$\\alpha$为一个比较小的非负数。\nLeaky ReLU函数解决了ReLU函数在输入为负的情况下产生的梯度消失问题。\n汇总 ","date":"2018-11-02T00:00:00Z","permalink":"https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/","title":"深度学习常用激活函数"},{"content":"梯度消失 梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。 通俗来讲就是多个小于1的参数多次连乘，导致结果非常小。如图中蓝色线条所示。\n梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率。\n梯度爆炸 在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为NaN值，再也无法更新。 通俗来讲就是多个大于1的参数多次连乘，导致结果非常大。\n","date":"2018-10-01T00:00:00Z","permalink":"https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/","title":"深度学习中的梯度爆炸和梯度消失"},{"content":"conda 创建环境:\n1 conda create -n py370 python=3.70 删除环境\n1 conda env remove --name py370 jupyter-lab jupyter-lab默认使用base环境，如果想在创建notebook时使用自己其他的环境，执行以下命令即可。执行完要重启jupyter-lab\n1 2 3 4 5 6 7 8 9 # linux source activate py370 # win conda activate py370 pip install jupyter notebook ipykernel ipython kernel install --user --name=py70 ","date":"2018-09-12T00:00:00Z","permalink":"https://charent.github.io/p/condajupyterlab%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/","title":"conda、jupyterlab使用技巧"},{"content":"TensorFolw 实战Google深度学习框架 用深度神经解决分类问题主要分为以下4个步骤：\n提取问题中实体的特征向量作为神经网络的输入 定义神经网络结构，并定义如何从神经网络的输入得到输出 训练神经网络，通过训练数据来调整神经网络中参数的取值 使用训练好的神经网络来预测未知数据 训练数据集、验证数据集、测试数据集最好来自于同一分布（训练集主要用户进行模型训练，验证集主要进行参数调整，测试集主要进行模型性能的评估） #####激活函数： 对于分类问题，现实中大部分数据都无法通过线性函数进行划分，使用激活函数实现去线性化，如：ReLU(max(x,0))函数，sigmoid函数，tanh函数。\nsigmoid函数和tanh函数很像，sigmoid函数使得函数输出范围在[0, 1]，左侧倒数从0开始趋向于0，容易造成梯度消失现象。而tanh函数的函数值域是[-1, 1]，特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果。与 sigmoid 的区别是，tanh 是 0 均值的，因此实际应用中 tanh 会比sigmoid 更好。\n损失函数： 交叉熵用来判断一个输出向量和期望的向量有多接近，刻画了连个概率分布直接的距离，是分类问题中使用比较广泛的损失函数，给定两个概率分布：p,q交叉熵: $$ H(p, q) = -∑ p(x) · log(q(x)) $$ , 求和范围为所有$x$,通过最小化交叉熵，使得正确分类和预测分类的概率分布接近。\n在原有的输出层增加一个额外的处理层，可以将神经网络变成一个概率分布，原始神经网络的输出被用作置信度来生成新的输出。\n分类问题： 经典损失函数：softmax，通过对原始输出层输出的数值进行求和，再用某一输出神经元的输出除以该和，得到预测值，所有输出神经元的预测值构成预测向量，再通过计算正确分类向量和预测向量的交叉熵，较小交叉熵的预测向量要优于较大交叉的预测向量\n对于回归问题（具体数值的预测，一般只有一个输出神经元），最常用的损失函数是均方误差（MSE）\n神经网络优化算法： 梯度下降法和反向传播法：先通过前向传播法计算得到预测值，并比较预测值和真实值之间的差距，再用反向传播算法计算每一个参数的梯度，最后根据每个参数的梯度和学习率使用梯度下降法更新每一个参数\n学习率设置：使用TensorFlow的指数衰减法（tf.train.exponential_decay），用较大学习率快速得到一个比较优的解，随着迭代次数增加，逐步减少学习率，使得模型在训练后更加稳定\n过拟合问题： 原因：1，训练数据太少；2，神经网络过于复杂。 解决办法:\n正则化：在损失函数中加入刻画模型复杂度的指标。是模型的复杂度，λ是模型复杂损失在总损失中的比例。 $$ j\u0026rsquo;(θ) = j(θ) + λ · R(w)，R(w) $$ 刻画模型复杂度的函数有两种，$w$为边上的权重，一是L1正则化： $$R(w) = ||w||1 = ∑|wi|$$ 二是L2正则化: $$ R(w) = ||w||2 ^ 2 = ∑|wi ^ 2| $$ L1正则化不可导，L2正则化比较复杂，一般L1和L2同时使用: $$ R(w) =∑ α · ||wi|| + (1 - α) · wi ^ 2 $$ 使用dropout，dropout使得神经网络的部分神经元的权值等于0或者接近于0，相当于删除该神经元=简化神经网络。tf.nn.dropout(layer,keep_prob )，keep_prob=1.0表示该层的神经元100%工作 滑动平均模型： 提高使用随机梯度下降法训练的神经网络模型在测试数据上的表现\n深度神经网络（DNN）： 即是最基本的多层全连接网络。前向传播法可以转化为基本的矩阵相乘来实现\n循环神经网络（RNN）： 当前状态的输出ot由当前状态的输入xt和上一时刻的隐含状态ht-1所决定的，在输出ot时，RNN还生成全新的隐含状态ht。常用沿时间反向传播训练方法对循环神经网络进行训练。\n卷积神经网络（CNN）： 解决全连接神经网络参数过多问题。一般组成：输入层-\u0026gt;( 卷积层+ -\u0026gt; 池化层? )+ -\u0026gt; 全连接层+ -\u0026gt; softmax层 -\u0026gt; 输出层。+表示一个或多个，?表示可有可无。加入池化层的作用是加快计算速度和防止过拟合（有研究指出加入池化层对模型效果影响不大，但主流CNN中一般都有池化层）。\n卷积层过滤器：使用3x3或者5x5尺寸的过滤器进行前向传播运算，过滤器每次移动（tensorflowz中可设置移动步数）都生成一个固定深度的卷积层。卷积层中过滤器的参数是共享的，可以大幅减少神经网络上的参数。 池化层：对卷积层进行分割，对分割的部分使用最大池化层或者平均池化层生成新的层。CNN比较适合处理图像，因为图像矩阵具有稠密性。高维、稀疏的向量不适合作为CNN的输入。 输入层是图像的像素点，且图片中的单个物体像素点是连续的，卷积核在输入层滑动时可以方便的提取到图片中的物体特征；另外，图片的像素点组成一个矩阵，这便于卷积核上下左右滑动提取特征。然而对于文本数据，文本中的词语却是离散的，比如“虽然……但是……”，这里“虽然”和“但是”显然具有关联的关系，但是它们之间却可能在句子中相隔很远，卷积核很难提取到这样的具有长距离依赖的词语之间的关系。 卷积操作其实就是卷积核矩阵和对应的输入层中一小块矩阵的点积相乘，卷积核通过权重共享的方式，按照步幅上下左右的在输入层滑动提取特征，以此将输入层做特征映射（features map）作为输出层，但在 NLP 中，由于词嵌入层中每一行都表示一个词语，我们在句子里面提取有利于分类的特征肯定是需要从词语（word）或者字符（char）级别去提取，也就是卷积宽口的宽度应该覆盖完全单个词向量，也就是 CNN 的卷积核宽度必须要等于词向量的维度 对于NPL，滑动窗口无法捕获远距离的特征\n自编码器（AE）： 其它： axis=0：矩阵的列 axis=1：矩阵的行\n","date":"2018-09-12T00:00:00Z","permalink":"https://charent.github.io/p/tensorflow%E7%AC%94%E8%AE%B0/","title":"Tensorflow笔记"},{"content":"预测-真实值定义 \u0026quot;\u0026quot; 预测值=1 预测值=0 真实值=1 True Positive(TP) False Negative(FN) 真实值=0 Positive (FP) True Negative(TN) 真假阳性定义： 真阳性True Positive，$TP$：样本的真实类别是正例，并且模型预测的结果也是正例 真阴性True Negative，$TN$：样本的真实类别是负例，并且模型将其预测成为负例 假阳性False Positive，$FP$：样本的真实类别是负例，但是模型将其预测成为正例 假阴性False Negative，$FN$：样本的真实类别是正例，但是模型将其预测成为负例 计算 准确度： $$ Accuracy = \\frac {TP+TN} {TP+TN+FN+TN} $$ 正确率: $$ Precision = \\frac {TP} {TP + FP)} $$ 真阳性率(True Positive Rate，TPR)，灵敏度(Sensitivity)，召回率: $$ Recall = \\frac {TP} {TP + FN} $$ 真阴性率(True Negative Rate，TNR)，特异度: $$ Specificity = \\frac {TN} {TN + FP} $$ 假阴性率(False Negatice Rate，FNR)，漏诊率( = 1 - 灵敏度) : $$ \\frac {FN} {TP + FN} = 1 - TPR $$ 假阳性率(False Positice Rate，FPR)，误诊率( = 1 - 特异度) ： $$ \\frac {FP} {FP + TN} = 1 - TNR $$ F1分数： $$ F1_{score} = \\frac {2 * TP} { 2 * TP + FP + FN} $$ ","date":"2018-09-12T00:00:00Z","permalink":"https://charent.github.io/p/%E5%87%86%E7%A1%AE%E7%8E%87%E5%8F%AC%E5%9B%9E%E7%8E%87f1%E5%88%86%E6%95%B0%E7%81%B5%E6%95%8F%E5%BA%A6%E7%89%B9%E5%BC%82%E5%BA%A6/","title":"准确率、召回率、F1分数、灵敏度、特异度"},{"content":"标题效果 这是一级标题 这是二级标题 这是三级标题 这是四级标题 这是五级标题 这是六级标题 字体效果 这是加粗的文字\n这是倾斜的文字`\n这是斜体加粗的文字\n这是加删除线的文字\n段落控制 这是首行缩进\n换行：\n1 \u0026lt;br/\u0026gt; 或者 空格 + 空格 + 回车 这是引用的内容\n这是引用的内容\n这是引用的内容\n分割线 超链接 简书\n百度\n列表 列表内容 列表内容 列表内容 缩进列表，敲3个空格 表格 注意：- + * 跟内容之间都要有一个空格\n表头 表头 表头 内容 内容 内容 内容 内容 内容 第二行分割表头和内容。\n有一个就行，为了对齐，多加了几个 文字默认居左 -两边加：表示文字居中 -右边加：表示文字居右 注：原生的语法两边都要用 | 包起来。此处省略 LaTeX 公式 $ 表示行内公式($之间不能有空格)：\n质能守恒方程可以用一个很简洁的方程式 $E=mc^2$ 来表达。\n$$ 表示整行公式：\n$$\\sum_{i=1}^n a_i=0$$\n$$f(x_1,x_x,\\ldots,x_n) = x_1^2 + x_2^2 + \\cdots + x_n^2 $$\n$$\\sum^{j-1}{k=0}{\\widehat{\\gamma}{kj} z_k}$$\n代码 单行代码内容\n1 2 3 4 function fun(){ echo \u0026#34;代码块\u0026#34;; } fun(); 流程图：\n1 2 3 4 5 6 7 st=\u0026gt;start: 开始 op=\u0026gt;operation: My Operation cond=\u0026gt;condition: Yes or No? e=\u0026gt;end st-\u0026gt;op-\u0026gt;cond cond(yes)-\u0026gt;e cond(no)-\u0026gt;op 图片 图片alt就是显示在图片下面的文字，相当于对图片内容的解释。 图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加\n1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2018-09-09T00:00:00Z","image":"https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/helena-hertz-wWZzXlDpMog-unsplash_hu_2307260c751d0e0b.jpg","permalink":"https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/","title":"Markdown模板"}]