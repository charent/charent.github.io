<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习 on Charent的博客</title>
        <link>https://charent.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 机器学习 on Charent的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Tue, 09 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://charent.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>深度学习反向传播推导</title>
        <link>https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/</link>
        <pubDate>Tue, 09 Jan 2024 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/</guid>
        <description>&lt;h1 id=&#34;反向传播推导&#34;&gt;反向传播推导
&lt;/h1&gt;&lt;p&gt;深度学习神经网络的前向传播大家都很清楚，对我来说，反向传播一直都是一知半解。今天重新复习一下。&lt;/p&gt;
&lt;p&gt;我们以&lt;code&gt;sigmoid&lt;/code&gt;函数作为一个计算图节点为例（加法乘法太简单了）。首先，&lt;code&gt;sigmoid函数&lt;/code&gt;的定义如下：
$$
y = \frac{1}{1 + e^{-x}}
$$
对$y$求$x$的偏导数：
$$ \begin {aligned}
\frac {\partial y}{ \partial x} &amp;amp;= \frac {-1}{(1 + e^{-x})^{2}} · (-e^{-x})  \
&amp;amp;= (\frac {1}{1 + e^{-x}}) ^2 · (e^{-x}) \
&amp;amp;= \frac {1}{1 + e^{-x}} · \frac {e^{-x}}{1 + e^{-x}} \
&amp;amp;= y · \frac {1 + e^{-x} - 1}{1 + e^{-x}} \
&amp;amp;= y · (1 - \frac {1}{1 + e^{-x}}) \
&amp;amp;= y · (1 - y)
\end {aligned}
$$
假设&lt;code&gt;sigomid&lt;/code&gt;节点前向传播的输出是$y$，反向传播到&lt;code&gt;sigomid&lt;/code&gt;节点的数值是$L$，根据链式求导法则，则该节点对$x$的梯度为 $ \frac {\partial L}{ \partial y} ·  \frac {\partial y}{ \partial x} = \frac {\partial L}{ \partial y} · y · (1 - y) $&lt;/p&gt;
&lt;p&gt;到这里可以写代码了：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;math&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# sigmoid func&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;math&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;backward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;out&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;dx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;out&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>机器学习特征选择之IV值计算</title>
        <link>https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/</link>
        <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/</guid>
        <description>&lt;h2 id=&#34;前言&#34;&gt;前言
&lt;/h2&gt;&lt;p&gt;  IV值（Information Value），信息价值指标，是评价一个特征好不好的指标之一。在金融风控领域广泛应用，尤其是在特征选择的场景下，会经常使用这个指标，特征选择得好不好，将直接影响模型的效果。&lt;/p&gt;
&lt;p&gt;  在金融风控领域，我们处理的更多是二分类问题，即是判断一个账号黑账户还是白账户。风险识别模型的效果很大程度上取决于对黑账户特征的分析，分析的黑账户越多，特征经验越丰富，模型效果越好。但是如何挑选分析得到的特征呢？用什么样的标准去判断这个特征能不能用呢？最方便的方法当然是拿所有的特征去训练一个模型，看看特征重要性，但这不是本文讨论的内容，我们要在特征做完之后，立刻判断这个特征能不能用，而不是等所有特征做完再去看特征重要性。用IV值！&lt;/p&gt;
&lt;h2 id=&#34;iv值先验知识1分箱&#34;&gt;IV值先验知识1：分箱
&lt;/h2&gt;&lt;p&gt;  特征分箱主要是为了降低变量的复杂性和减少变量噪音对模型的影响，提高自变量和因变量的相关度，从而使模型更加稳定。&lt;/p&gt;
&lt;p&gt;  监督学习中的分箱常用的有Split分箱和Merge分箱。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Split 分箱是一段连续的值分为若干段离散的值，Split分箱和决策树比较相似，切分点的选择指标主要有信息熵、gini 指数等。比如，年龄可以分为 ≤18，19-24，25-35，35-54，≥55。&lt;/li&gt;
&lt;li&gt;Merge分箱则通过计算两个从小到大排序的数值的卡方值，将最小卡方值的相邻组合并为一组，再重新排序，重新计算卡方值，合并组，直到计算出的卡方值都不低于事先设定的阈值, 或者分组数达到一定的数量。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;iv值先验知识2woe编码&#34;&gt;IV值先验知识2：WOE编码
&lt;/h2&gt;&lt;p&gt;  分箱之后我们便得到了一系列的离散变量，接下来需要对变量进行编码，将离散变量转化为连续变量。WOE编码是评分卡模型常用的编码方式。&lt;br&gt;
  WOE称为证据权重(weight of evidence)，是一种有监督的编码方式，将预测类别的集中度的属性作为编码的数值。对于自变量第 $i$ 箱的WOE值为：
$$
WOE_i = ln(\frac{P_{i_1}} {P_{i_0}} ) = ln(\frac {B_i / B_T} {
G_i / G_T
})
$$
  其中:&lt;br&gt;
  $P_{i_1}$ 是第 $i$ 箱中黑账户占所有黑账户比例；&lt;br&gt;
  $P_{i_0}$ 是第 $i$ 箱中白账户占所有白账户的比例；&lt;br&gt;
  $B_i$ 是第 $i$ 箱中黑账户的个数；&lt;br&gt;
  $G_i$ 是第 $i$ 箱中白账户的个数；&lt;br&gt;
  $B_T$ 是所有黑账户个数；&lt;br&gt;
  $G_T$ 是所有白账户个数。&lt;/p&gt;
&lt;p&gt;   变换以后可以看出，WOE也可以理解为当前分箱中黑账户和白账户的比值，和所有样本中这个比值的差异。WOE越大，这种差异越大，当前分组里的黑账户的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。当分箱中黑账户和白账户的比例等于样本中所有黑账户和白账户的比值时，说明这个分箱没有预测能力，即WOE=0。&lt;/p&gt;
&lt;h3 id=&#34;qa为什么不直接使用原始数据中的连续变量而是先分箱为离散变量再将离散变量转换为连续变量woe编码&#34;&gt;QA：为什么不直接使用原始数据中的连续变量，而是先分箱为离散变量，再将离散变量转换为连续变量WOE编码？
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;  WOE可以把相对于预测变量显现非线性的特征转换为线性。例如：很多黑账户的年龄在19-24岁，随着年龄的增长黑账户逐渐变小，黑账户数量（纵坐标，数值为黑账户占总账户比）和年龄是一个非线性的关系。分箱后转换为WOE编码，黑账户数量（纵坐标，数值为黑账户占总账户比）和WOE值呈线性关系。如下图所示，对于机器学习模型而言，线性关系更容易区分黑白账户。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/before_woe.png&#34;
	width=&#34;240&#34;
	height=&#34;204&#34;
	srcset=&#34;https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/before_woe_hu_7d6f9a8c5b314921.png 480w, https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/before_woe_hu_aa2ae075814f10bc.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;WOE变换前&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;282px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/after_woe.png&#34;
	width=&#34;240&#34;
	height=&#34;208&#34;
	srcset=&#34;https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/after_woe_hu_51a5f76a1be60090.png 480w, https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/after_woe_hu_157eac1040cc0c8f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;WOE变换后&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;115&#34;
		data-flex-basis=&#34;276px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;iv值计算&#34;&gt;IV值计算
&lt;/h2&gt;&lt;p&gt;  特征变量中第 $i$ 个分箱对应的IV值的计算公式为：
$$
IV_i = ( \frac{B_i} {B_T} - \frac{G_i} {G_T} ) \times ln(\frac {B_i / B_T} {
G_i / G_T})   \
= ( \frac{B_i} {B_T} - \frac{G_i} {G_T} ) \times  WOE_i
$$&lt;/p&gt;
&lt;p&gt;  变量中第 $i$ 个分箱对应的IV值的计算公式为：
$$
IV = \sum \limits _{i = 1}^n IV_i
$$&lt;/p&gt;
&lt;p&gt;特别地，如果特征没有进行分箱操作，相当于只有一个像，上面公式的i和i都等于1。
IV值的取值范围是[0,+∞)，当分箱中只包含白账户或只包含黑账户时，IV = +∞，当分箱中黑白账户比例等于整体黑白账户比例时，IV为0。&lt;/p&gt;
&lt;p&gt;IV值计算完成后，即可根据IV值的大小判断特征是否对有用（特征的预测能力是否强）。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;IV值范围&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;预测能力&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;＜0.02&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;无效特征，无预测能力&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;[0.02, 0.10)&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;弱效果特征，预测能力弱&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;[0.10, 0.50)&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;有效特征，预测能力中等&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;≥0.50&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;强特征，预测能力强&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        <item>
        <title>小样本学习</title>
        <link>https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;h1 id=&#34;孪生网络siamese-network&#34;&gt;孪生网络（Siamese Network）
&lt;/h1&gt;&lt;p&gt;本质上还是一个网络，但是每次都是两个样本输入到网络中，再计算网络两个输出的相似度。数据集划分为：训练集，支持集，测试集。训练集：从同一类别下采样相同的两个句子作为正样本，从不同的类别下采样两个句子作为负样本，保证正负样本对的数量接近1：1，然后输入到孪生网络中作为一个二分类的任务来度量两个句子之间的距离。&lt;/p&gt;
&lt;h1 id=&#34;induction-network感应网络&#34;&gt;Induction Network（感应网络）
&lt;/h1&gt;&lt;p&gt;训练集中，每一个episode的时候，都随机选择C个类（训练集中的类别个数大于C），然后每一个类别都同样随机选择K个样本，这样每一个episode中的数据样本个数便是C * K个，这CK个样本组成support set S，此外，再从剩余的样本中随机选择n个样本作为query set Q，每一个episode都在这样选择出来的S和Q上进行训练
网络由三个模块组成：编码器模块，归纳模块和关系模块
&lt;img src=&#34;https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/1.png&#34;
	width=&#34;762&#34;
	height=&#34;438&#34;
	srcset=&#34;https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/1_hu_f6d243855d093fb5.png 480w, https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/1_hu_1b442fd071b87df3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;173&#34;
		data-flex-basis=&#34;417px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;编码器模块&#34;&gt;编码器模块：
&lt;/h2&gt;&lt;p&gt;相当于一个encoder。可以利用CNN，LSTM和Transformer等等，在阿里的论文《Few-Shot Text Classification with Induction Network》中论使用LSTM，简单讲就是：针对每一个样本，将LSTM各个时刻的隐层输出h，做一次self-attention，最后得到一个向量e。&lt;/p&gt;
&lt;h2 id=&#34;归纳模块&#34;&gt;归纳模块：
&lt;/h2&gt;&lt;p&gt;借用了胶囊网络的动态路由概念，将每一个类别中的样本表征，最后转化凝练成为class-level的表征。
&lt;img src=&#34;https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/2.png&#34;
	width=&#34;742&#34;
	height=&#34;449&#34;
	srcset=&#34;https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/2_hu_5b8536054dff882d.png 480w, https://charent.github.io/p/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/2_hu_5ea0454fc9f33568.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;396px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;关系模块&#34;&gt;关系模块：
&lt;/h2&gt;&lt;p&gt;在归纳模块生成类向量C^i并且查询集中的每个查询文本被编码器模块编码为查询向量e^q之后，下一步就是计算每对查询向量和类向量之间的相关性，输出区间在[0,1]之间的得分&lt;/p&gt;
&lt;h1 id=&#34;原型网络prototypical-network&#34;&gt;原型网络（Prototypical Network）
&lt;/h1&gt;&lt;p&gt;论文《Prototypical Networks for Few-shot Learning》
给定一个训练时的train set，测试时的support set和query。support set 包含C个类别，每个类别下含有K个样本。train set 包含M个类别，每个类别下含有N个样本。为了在训练时期模拟测试时的场景，我们在训练时构造一系列的episode，每个episode实际上就是一个meta task。那该怎么构造这样一个episode呢？从train set中随机抽取C个类别，然后从每个类别中随机抽取K个样本，构造训练时期的support set，这样的问题也称为C-way K-shot问题，接着从另外N-K个样本中选取n个样本作为训练时期的query。构造一系列这样的episode来训练网络&lt;/p&gt;
&lt;h1 id=&#34;关系网络relation-network&#34;&gt;关系网络（Relation Network）
&lt;/h1&gt;&lt;p&gt;论文《Learning to Compare: Relation Network for Few-Shot Learning》
整个训练和预测时的方法和原型网络是一样的。其主要创新点在于之前的网络都会给定一个确定的距离度量函数，然而作者认为没有一个确定的距离函数能作为所有类别的最佳度量函数，因此作者让网络自己去学习一个这样的度量函数，这里的Relation network就是通过关系网络来度量query和各类别之间的关系&lt;/p&gt;
</description>
        </item>
        <item>
        <title>多标签分类</title>
        <link>https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/</link>
        <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/</guid>
        <description>&lt;h1 id=&#34;多标签分类问题&#34;&gt;多标签分类问题
&lt;/h1&gt;&lt;p&gt;有多个类别，但每个样例可能对应多个类别，因此这些问题被称为多类分类问题。
通过一份体检报告判断一个人是否患有以下五种病：有序排列——&lt;code&gt;[高血压，高血糖，肥胖，肺结核，冠心病]&lt;/code&gt;，一个样本&lt;code&gt;[1,0,1,0,0]&lt;/code&gt; ，其中1代表该位置的患病，0代表没患病。所以这个label的含义：患者有高血压和肥胖。&lt;/p&gt;
&lt;h2 id=&#34;解决多标签分类问题的方法&#34;&gt;解决多标签分类问题的方法：
&lt;/h2&gt;&lt;p&gt;基本上，有三种方法来解决一个多标签分类问题，即:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;问题转换&lt;/li&gt;
&lt;li&gt;改编算法&lt;/li&gt;
&lt;li&gt;集成方法&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;问题转换&#34;&gt;问题转换：
&lt;/h3&gt;&lt;p&gt;在这个方法中，我们将尝试把多标签问题转换为单标签问题。这种方法可以用三种不同的方式进行:
二元关联（Binary Relevance）
分类器链（Classifier Chains）
标签Powerset（Label Powerset）&lt;/p&gt;
&lt;p&gt;二元关联（Binary Relevance）
这是最简单的技术，它基本上把每个标签当作单独的一个类分类问题。例如，让我们考虑如下所示的一个案例。我们有这样的数据集，X是独立的特征，Y是目标变量。
&lt;img src=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/1.png&#34;
	width=&#34;172&#34;
	height=&#34;159&#34;
	srcset=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/1_hu_db2a10919fbf3c45.png 480w, https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/1_hu_ba7bf4d17fe8dacf.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;108&#34;
		data-flex-basis=&#34;259px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在二元关联中，这个问题被分解成4个不同的类分类问题，如下图所示。
&lt;img src=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/2.png&#34;
	width=&#34;306&#34;
	height=&#34;157&#34;
	srcset=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/2_hu_f5b0807a5ad3bfb3.png 480w, https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/2_hu_2cd4d62d6a57eb4c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;467px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;分类器链classifier-chains&#34;&gt;分类器链（Classifier Chains）
&lt;/h3&gt;&lt;p&gt;在这种情况下，第一个分类器只在输入数据上进行训练，然后每个分类器都在输入空间和链上的所有之前的分类器上进行训练。
让我们试着通过一个例子来理解这个问题。在下面给出的数据集里，我们将X作为输入空间，而Y作为标签。
&lt;img src=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/3.png&#34;
	width=&#34;221&#34;
	height=&#34;114&#34;
	srcset=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/3_hu_682b93acd4e20e58.png 480w, https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/3_hu_aa857f8857321dbc.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;193&#34;
		data-flex-basis=&#34;465px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在分类器链中，这个问题将被转换成4个不同的标签问题，就像下面所示。黄色部分是输入空间，白色部分代表目标变量。这与二元关联非常相似，唯一的区别在于它是为了保持标签相关性而形成的。
&lt;img src=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/4.png&#34;
	width=&#34;619&#34;
	height=&#34;122&#34;
	srcset=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/4_hu_64f43cb3d4f374ba.png 480w, https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/4_hu_70c5818eed021c7c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;507&#34;
		data-flex-basis=&#34;1217px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;标签powersetlabel-powerset&#34;&gt;标签Powerset（Label Powerset）
&lt;/h3&gt;&lt;p&gt;在这方面，我们将问题转化为一个多类问题，一个多类分类器在训练数据中发现的所有唯一的标签组合上被训练。让我们通过一个例子来理解它。
&lt;img src=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/5.png&#34;
	width=&#34;225&#34;
	height=&#34;196&#34;
	srcset=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/5_hu_ca078b6ec405fc6.png 480w, https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/5_hu_7440ff3f3f42f61f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;114&#34;
		data-flex-basis=&#34;275px&#34;
	
&gt;
在这一点上，我们发现x1和x4有相同的标签。同样的，x3和x6有相同的标签。因此，标签powerset将这个问题转换为一个单一的多类问题，如下所示。
&lt;img src=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/6.png&#34;
	width=&#34;101&#34;
	height=&#34;184&#34;
	srcset=&#34;https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/6_hu_78354a507b26242c.png 480w, https://charent.github.io/p/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/6_hu_714e424f9c1ff2d9.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;6&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;54&#34;
		data-flex-basis=&#34;131px&#34;
	
&gt;
因此，标签powerset给训练集中的每一个可能的标签组合提供了一个独特的类。&lt;/p&gt;
&lt;h2 id=&#34;深度学习方法&#34;&gt;深度学习方法：
&lt;/h2&gt;&lt;h3 id=&#34;模型输入输出&#34;&gt;模型输入输出
&lt;/h3&gt;&lt;p&gt;假设我们有一个体检疾病判断任务：通过一份体检报告判断一个人是否患有以下五种病：有序排列——[高血压，高血糖，肥胖，肺结核，冠心病]
输入：一份体检报告
输出：&lt;code&gt;[1,0,1,0,0 ]&lt;/code&gt; ，其中1代表该位置的患病，0代表没患病。所以这个&lt;code&gt;label&lt;/code&gt;的含义：患者有高血压和肥胖。&lt;/p&gt;
&lt;h3 id=&#34;模型架构&#34;&gt;模型架构
&lt;/h3&gt;&lt;p&gt;接下来如何建立模型呢:
当然可以对&lt;code&gt;label&lt;/code&gt;的每一个维度分别进行建模，训练5个二分类器。
但是这样不仅是的&lt;code&gt;label&lt;/code&gt;之间的依赖关系被破坏，而且还耗时耗力。&lt;/p&gt;
&lt;p&gt;接下来我们还是来看看深度神经网络是如何应用于此问题的。其架构如下：
采用神经网络做特征提取器，这部分不需要多解释，就是一个深度学习网络；
采用&lt;code&gt;sigmoid&lt;/code&gt;做输出层的激活函数，若做体检疾病判断任务的话输出层是5个节点对应一个5维向量，这里没有采用&lt;code&gt;softmax&lt;/code&gt;，就是希望&lt;code&gt;sigmoid&lt;/code&gt;对每一个节点的值做一次激活，从而输出每个节点分别是 1 概率；&lt;/p&gt;
&lt;p&gt;采用&lt;code&gt;binary_crossentropy&lt;/code&gt;损失函数函数，这样使得模型在训练过程中不断降低&lt;code&gt;output&lt;/code&gt;和&lt;code&gt;label&lt;/code&gt;之间的交叉熵。其实就相当于模型使&lt;code&gt;label&lt;/code&gt;为1的节点的输出值更靠近1，&lt;code&gt;label&lt;/code&gt;为0的节点的输出值更靠近0。&lt;/p&gt;
&lt;p&gt;有点类似 Structure Learing ，最终模型的输出就是一个结构序列。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>推荐系统衡量方法</title>
        <link>https://charent.github.io/p/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%A1%A1%E9%87%8F%E6%96%B9%E6%B3%95/</link>
        <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%A1%A1%E9%87%8F%E6%96%B9%E6%B3%95/</guid>
        <description>&lt;h1 id=&#34;ab-test&#34;&gt;AB Test：
&lt;/h1&gt;&lt;p&gt;A/B 测试是为Web或App界面或流程制作两个（A/B）或多个（A/B/n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用。&lt;/p&gt;
&lt;h1 id=&#34;点击通过率click-through-ratectr&#34;&gt;点击通过率（Click-through Rate，CTR）:
&lt;/h1&gt;&lt;p&gt;一般指网络广告的点击到达率，即该广告的实际点击次数除以广告的展现量，即clicks/views。反映了网页上某一内容的受关注程度，常常用来衡量广告的吸引程度&lt;/p&gt;
&lt;h1 id=&#34;roc曲线&#34;&gt;ROC曲线
&lt;/h1&gt;&lt;p&gt;接受者操作特性曲线（receiver operating characteristic curve，简称ROC曲线）。曲线的横坐标为假阳性率（False Positive Rate, FPR）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;FPR=\frac {FP} {(FP+TN)}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;N是真实负样本的个数，
FP是N个负样本中被分类器预测为正样本的个数。
纵坐标为真阳性率（True Positive Rate, TPR）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;TPR=\frac {TP}{P}=\frac {TP} {(TP+FN)}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其中，P是真实正样本的个数，TP是P个正样本中被分类器预测为正样本的个数。&lt;/p&gt;
&lt;h1 id=&#34;auc-roc曲线下方的面积大小&#34;&gt;AUC （ROC曲线下方的面积大小）：
&lt;/h1&gt;&lt;p&gt;AUC（Area Under Curve）被定义为ROC曲线下与坐标轴围成的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。AUC越接近1.0，检测方法真实性越高;等于0.5时，则真实性最低，无应用价值。我们往往使用AUC值作为模型的评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好&lt;/p&gt;
</description>
        </item>
        <item>
        <title>负样本（负采样）</title>
        <link>https://charent.github.io/p/%E8%B4%9F%E6%A0%B7%E6%9C%AC%E8%B4%9F%E9%87%87%E6%A0%B7/</link>
        <pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E8%B4%9F%E6%A0%B7%E6%9C%AC%E8%B4%9F%E9%87%87%E6%A0%B7/</guid>
        <description>&lt;h1 id=&#34;负采样&#34;&gt;负采样
&lt;/h1&gt;&lt;p&gt;自然语言处理领域中，判断两个单词是不是一对上下文词（context）与目标词（target），如果是一对，则是正样本，如果不是一对，则是负样本。
采样得到一个上下文词和一个目标词，生成一个正样本（positive example），生成一个负样本（negative example），则是用与正样本相同的上下文词，再在字典中随机选择一个单词，这就是负采样（negative sampling）。&lt;/p&gt;
&lt;p&gt;负采样是为了解决类别太多的一种折中方案，样本只是给模型训练提供信息的，那负样本的选择肯定是选信息量大的那些，比如一些模型的决策边界，如果有充足的样本就能学的比较好，如果负样本离分离边界太远，那其实提供不了太多有用信息，甚至会误导模型使其有偏。&lt;/p&gt;
&lt;p&gt;还有就是一些任务比如预测任务，负采样可能会使其偏度较大，比如点击率预估，本来样本点击率为0.01，负采样使正负样本比例1:9，那最后样本平均点击率就为0.1，这种任务如果一定要负采样肯定要进行一定的修正。
如果是一般的任务，其实负样本选择对效果的影响很大。主要看数据分布，分布波动较大，样本噪声高的任务，负采样很难。
其原理就是正常正负样本对参数影响的原理。而且一般都不是随机负采样，都是按照一定权重方法采样，进而也揭示了负采样其实不能随便采。&lt;/p&gt;
&lt;p&gt;negative sampling在以下几种情况可能会有不一样的结果。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;样本信息过分冗余，通过negative sampling可以在相同机器资源的情况下提高训练速度，而且对效果影响很有限，这对于有限预算下是很重要的。&lt;/li&gt;
&lt;li&gt;负样本不能有效反应用户真实意图的情况下，negative sampling可能会带来收益，比如有一些场景用户很可能大部分都没有看到而导致的负样本采集；&lt;/li&gt;
&lt;li&gt;对于不同的问题也可能会不太一样，比如说implicit和explicit的问题，implict的feedback本身也是有折损的，也就是不点击不代表不喜欢，点击也不代表一定喜欢，需要考虑的信号就需要更仔细的看了。&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>准确率、召回率、F1分数、灵敏度、特异度</title>
        <link>https://charent.github.io/p/%E5%87%86%E7%A1%AE%E7%8E%87%E5%8F%AC%E5%9B%9E%E7%8E%87f1%E5%88%86%E6%95%B0%E7%81%B5%E6%95%8F%E5%BA%A6%E7%89%B9%E5%BC%82%E5%BA%A6/</link>
        <pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E5%87%86%E7%A1%AE%E7%8E%87%E5%8F%AC%E5%9B%9E%E7%8E%87f1%E5%88%86%E6%95%B0%E7%81%B5%E6%95%8F%E5%BA%A6%E7%89%B9%E5%BC%82%E5%BA%A6/</guid>
        <description>&lt;h4 id=&#34;预测-真实值定义&#34;&gt;预测-真实值定义
&lt;/h4&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&amp;quot;&amp;quot;&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;预测值=1&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;预测值=0&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;真实值=1&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;True Positive(TP)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;False Negative(FN)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;真实值=0&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Positive (FP)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;True Negative(TN)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;真假阳性定义&#34;&gt;真假阳性定义：
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;真阳性&lt;code&gt;True Positive&lt;/code&gt;，$TP$：样本的真实类别是正例，并且模型预测的结果也是正例&lt;/li&gt;
&lt;li&gt;真阴性&lt;code&gt;True Negative&lt;/code&gt;，$TN$：样本的真实类别是负例，并且模型将其预测成为负例&lt;/li&gt;
&lt;li&gt;假阳性&lt;code&gt;False Positive&lt;/code&gt;，$FP$：样本的真实类别是负例，但是模型将其预测成为正例&lt;/li&gt;
&lt;li&gt;假阴性&lt;code&gt;False Negative&lt;/code&gt;，$FN$：样本的真实类别是正例，但是模型将其预测成为负例&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;计算&#34;&gt;计算
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;准确度：
$$ Accuracy = \frac {TP+TN} {TP+TN+FN+TN} $$&lt;/li&gt;
&lt;li&gt;正确率:
$$ Precision = \frac {TP}  {TP + FP)} $$&lt;/li&gt;
&lt;li&gt;真阳性率(True Positive Rate，TPR)，灵敏度(Sensitivity)，召回率:
$$ Recall = \frac {TP}  {TP + FN} $$&lt;/li&gt;
&lt;li&gt;真阴性率(True Negative Rate，TNR)，特异度:
$$ Specificity = \frac {TN} {TN + FP} $$&lt;/li&gt;
&lt;li&gt;假阴性率(False Negatice Rate，FNR)，漏诊率( = 1 - 灵敏度) :
$$ \frac {FN} {TP + FN} = 1 - TPR $$&lt;/li&gt;
&lt;li&gt;假阳性率(False Positice Rate，FPR)，误诊率( = 1 - 特异度) ：
$$ \frac {FP} {FP + TN} = 1 - TNR $$&lt;/li&gt;
&lt;li&gt;F1分数：
$$ F1_{score} = \frac {2 * TP} { 2 * TP + FP + FN} $$&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
