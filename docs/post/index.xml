<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Charent的博客</title>
        <link>https://charent.github.io/post/</link>
        <description>Recent content in Posts on Charent的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 16 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://charent.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习特征选择之IV值计算</title>
        <link>https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/</link>
        <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/</guid>
        <description>&lt;img src="https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/before_woe.png" alt="Featured image of post 机器学习特征选择之IV值计算" /&gt;&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;  IV值（Information Value），信息价值指标，是评价一个特征好不好的指标之一。在金融风控领域广泛应用，尤其是在特征选择的场景下，会经常使用这个指标，特征选择得好不好，将直接影响模型的效果。&lt;/p&gt;
&lt;p&gt;  在金融风控领域，我们处理的更多是二分类问题，即是判断一个账号黑账户还是白账户。风险识别模型的效果很大程度上取决于对黑账户特征的分析，分析的黑账户越多，特征经验越丰富，模型效果越好。但是如何挑选分析得到的特征呢？用什么样的标准去判断这个特征能不能用呢？最方便的方法当然是拿所有的特征去训练一个模型，看看特征重要性，但这不是本文讨论的内容，我们要在特征做完之后，立刻判断这个特征能不能用，而不是等所有特征做完再去看特征重要性。用IV值！&lt;/p&gt;
&lt;h2 id=&#34;iv值先验知识1分箱&#34;&gt;IV值先验知识1：分箱&lt;/h2&gt;
&lt;p&gt;  特征分箱主要是为了降低变量的复杂性和减少变量噪音对模型的影响，提高自变量和因变量的相关度，从而使模型更加稳定。&lt;/p&gt;
&lt;p&gt;  监督学习中的分箱常用的有Split分箱和Merge分箱。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Split 分箱是一段连续的值分为若干段离散的值，Split分箱和决策树比较相似，切分点的选择指标主要有信息熵、gini 指数等。比如，年龄可以分为 ≤18，19-24，25-35，35-54，≥55。&lt;/li&gt;
&lt;li&gt;Merge分箱则通过计算两个从小到大排序的数值的卡方值，将最小卡方值的相邻组合并为一组，再重新排序，重新计算卡方值，合并组，直到计算出的卡方值都不低于事先设定的阈值, 或者分组数达到一定的数量。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;iv值先验知识2woe编码&#34;&gt;IV值先验知识2：WOE编码&lt;/h2&gt;
&lt;p&gt;  分箱之后我们便得到了一系列的离散变量，接下来需要对变量进行编码，将离散变量转化为连续变量。WOE编码是评分卡模型常用的编码方式。&lt;br&gt;
  WOE称为证据权重(weight of evidence)，是一种有监督的编码方式，将预测类别的集中度的属性作为编码的数值。对于自变量第 $i$ 箱的WOE值为：
$$
WOE_i = ln(\frac{P_{i_1}} {P_{i_0}} ) = ln(\frac {B_i / B_T} {
G_i / G_T
})
$$
  其中:&lt;br&gt;
  $P_{i_1}$ 是第 $i$ 箱中黑账户占所有黑账户比例；&lt;br&gt;
  $P_{i_0}$ 是第 $i$ 箱中白账户占所有白账户的比例；&lt;br&gt;
  $B_i$ 是第 $i$ 箱中黑账户的个数；&lt;br&gt;
  $G_i$ 是第 $i$ 箱中白账户的个数；&lt;br&gt;
  $B_T$ 是所有黑账户个数；&lt;br&gt;
  $G_T$ 是所有白账户个数。&lt;/p&gt;
&lt;p&gt;   变换以后可以看出，WOE也可以理解为当前分箱中黑账户和白账户的比值，和所有样本中这个比值的差异。WOE越大，这种差异越大，当前分组里的黑账户的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。当分箱中黑账户和白账户的比例等于样本中所有黑账户和白账户的比值时，说明这个分箱没有预测能力，即WOE=0。&lt;/p&gt;
&lt;h3 id=&#34;qa为什么不直接使用原始数据中的连续变量而是先分箱为离散变量再将离散变量转换为连续变量woe编码&#34;&gt;QA：为什么不直接使用原始数据中的连续变量，而是先分箱为离散变量，再将离散变量转换为连续变量WOE编码？&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;  WOE可以把相对于预测变量显现非线性的特征转换为线性。例如：很多黑账户的年龄在19-24岁，随着年龄的增长黑账户逐渐变小，黑账户数量（纵坐标，数值为黑账户占总账户比）和年龄是一个非线性的关系。分箱后转换为WOE编码，黑账户数量（纵坐标，数值为黑账户占总账户比）和WOE值呈线性关系。如下图所示，对于机器学习模型而言，线性关系更容易区分黑白账户。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/before_woe.png&#34;
	width=&#34;240&#34;
	height=&#34;204&#34;
	srcset=&#34;https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/before_woe_huf43de42c2f8c2ecec84dabaaee49e564_33528_480x0_resize_box_3.png 480w, https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/before_woe_huf43de42c2f8c2ecec84dabaaee49e564_33528_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;WOE变换前&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;282px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/after_woe.png&#34;
	width=&#34;240&#34;
	height=&#34;208&#34;
	srcset=&#34;https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/after_woe_hu817d1b854ba461d42c82dc778a618177_24034_480x0_resize_box_3.png 480w, https://charent.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8Biv%E5%80%BC%E8%AE%A1%E7%AE%97/after_woe_hu817d1b854ba461d42c82dc778a618177_24034_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;WOE变换后&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;115&#34;
		data-flex-basis=&#34;276px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;iv值计算&#34;&gt;IV值计算&lt;/h2&gt;
&lt;p&gt;  特征变量中第 $i$ 个分箱对应的IV值的计算公式为：
$$
IV_i = ( \frac{B_i} {B_T} - \frac{G_i} {G_T} ) \times ln(\frac {B_i / B_T} {
G_i / G_T})   \
= ( \frac{B_i} {B_T} - \frac{G_i} {G_T} ) \times  WOE_i
$$&lt;/p&gt;
&lt;p&gt;  变量中第 $i$ 个分箱对应的IV值的计算公式为：
$$
IV = \sum \limits _{i = 1}^n IV_i
$$&lt;/p&gt;
&lt;p&gt;特别地，如果特征没有进行分箱操作，相当于只有一个像，上面公式的i和i都等于1。
IV值的取值范围是[0,+∞)，当分箱中只包含白账户或只包含黑账户时，IV = +∞，当分箱中黑白账户比例等于整体黑白账户比例时，IV为0。&lt;/p&gt;
&lt;p&gt;IV值计算完成后，即可根据IV值的大小判断特征是否对有用（特征的预测能力是否强）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;IV值范围&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;预测能力&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;＜0.02&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;无效特征，无预测能力&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[0.02, 0.10)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;弱效果特征，预测能力弱&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[0.10, 0.50)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;有效特征，预测能力中等&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;≥0.50&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;强特征，预测能力强&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        <item>
        <title>深度学习常用激活函数</title>
        <link>https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link>
        <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</guid>
        <description>&lt;img src="https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/%E6%B1%87%E6%80%BB%E5%9B%BE.png" alt="Featured image of post 深度学习常用激活函数" /&gt;&lt;h2 id=&#34;为什么需要激活函数&#34;&gt;为什么需要激活函数？&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。&lt;/li&gt;
&lt;li&gt;激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。&lt;/li&gt;
&lt;li&gt;激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;为什么激活函数需要非线性函数&#34;&gt;为什么激活函数需要非线性函数？&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。&lt;/li&gt;
&lt;li&gt;使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;如何选择激活函数&#34;&gt;如何选择激活函数&lt;/h2&gt;
&lt;p&gt;  选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。&lt;/li&gt;
&lt;li&gt;如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。
sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。&lt;/li&gt;
&lt;li&gt;tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。&lt;/li&gt;
&lt;li&gt;ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。&lt;/li&gt;
&lt;li&gt;如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;  就我个人使用经验而言，非输出层一般使用Relu，复杂网络也会考虑Leaky Relu，输出层：如果是二分类，sigmoid无疑了（当然也可以用softmax），多分类则是softmax。&lt;/p&gt;
&lt;h2 id=&#34;激活函数分类&#34;&gt;激活函数分类&lt;/h2&gt;
&lt;p&gt;在神经网络计算中，输入X会先进行一个线性变换，&lt;/p&gt;
&lt;p&gt;$$ y = W · X + b $$&lt;/p&gt;
&lt;p&gt;之后再进行一个非线性变换，即是y通过一个非线性的激活函数：
$$ output=g(y) $$&lt;/p&gt;
&lt;p&gt;$g(y)$ 为非线性激活函数。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;sigmoid函数&lt;br&gt;
sgmoid函数的计算公式为：&lt;/p&gt;
&lt;p&gt;$$ g(x)= \frac{1} {1 + e^{-x} }  $$&lt;/p&gt;
&lt;p&gt;sigmoid函数缺点：当 x 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 $g′(x)$ 将接近 0 。这会导致权重 W 的梯度将接近 0 ，使得梯度更新十分缓慢，即梯度消失。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmiod.png&#34;
	width=&#34;237&#34;
	height=&#34;192&#34;
	srcset=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmiod_hu609d3bebef95135389685bac4d427f15_19341_480x0_resize_box_3.png 480w, https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmiod_hu609d3bebef95135389685bac4d427f15_19341_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;sigmoid函数&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;123&#34;
		data-flex-basis=&#34;296px&#34;
	
&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;tanh函数&lt;br&gt;
tanh函数的计算公式为:
$$ g(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}  $$&lt;br&gt;
tanh函数的缺点同sigmoid函数的第一个缺点一样，当 x 很大或很小时，$g′(x)$ 接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh.png&#34;
	width=&#34;237&#34;
	height=&#34;192&#34;
	srcset=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh_hu609d3bebef95135389685bac4d427f15_19341_480x0_resize_box_3.png 480w, https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh_hu609d3bebef95135389685bac4d427f15_19341_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;tanh函数&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;123&#34;
		data-flex-basis=&#34;296px&#34;
	
&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;ReLU函数&lt;br&gt;
ReLU函数计算公式为：&lt;br&gt;
$$
g(x) = \left{
\begin{array}{rcl}
x   &amp;amp;&amp;amp;  {x &amp;gt; 0} \
0   &amp;amp;&amp;amp;  {x \le 0}
\end{array} \right.
$$
ReLU函数的优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在输入为正数的时候（对于大多数输入 x 空间来说），不存在梯度消失问题。&lt;/li&gt;
&lt;li&gt;计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。sigmod和tanh要计算指数，计算速度会比较慢。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ReLU函数的缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当输入为负时，梯度为0，会产生梯度消失问题。
&lt;img src=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu.png&#34;
	width=&#34;275&#34;
	height=&#34;213&#34;
	srcset=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu_hucacf704b34715820a22096d6ab3000df_17660_480x0_resize_box_3.png 480w, https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu_hucacf704b34715820a22096d6ab3000df_17660_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;relu函数&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;129&#34;
		data-flex-basis=&#34;309px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leaky ReLU函数&lt;br&gt;
Leaky Relu函数计算公式：
$$
g(x) = \left {
\begin{array}{cl}
x   &amp;amp;&amp;amp;  {x &amp;gt; 0} \
\alpha \times x   &amp;amp;&amp;amp;  {x \le 0}
\end{array}
\right.
$$
其中，$\alpha$为一个比较小的非负数。&lt;br&gt;
Leaky ReLU函数解决了ReLU函数在输入为负的情况下产生的梯度消失问题。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/leaky_relu.png&#34;
	width=&#34;244&#34;
	height=&#34;205&#34;
	srcset=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/leaky_relu_huc3b8103c2edee6001342c8a151eb0a27_20089_480x0_resize_box_3.png 480w, https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/leaky_relu_huc3b8103c2edee6001342c8a151eb0a27_20089_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;leaky_relu函数&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;285px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>深度学习中的梯度爆炸和梯度消失</title>
        <link>https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/</link>
        <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/</guid>
        <description>&lt;img src="https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/clipboard.png" alt="Featured image of post 深度学习中的梯度爆炸和梯度消失" /&gt;&lt;h2 id=&#34;梯度消失&#34;&gt;梯度消失&lt;/h2&gt;
&lt;p&gt;  梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。
通俗来讲就是多个小于1的参数多次连乘，导致结果非常小。如图中蓝色线条所示。&lt;/p&gt;
&lt;p&gt;  梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/clipboard.png&#34;
	width=&#34;800&#34;
	height=&#34;600&#34;
	srcset=&#34;https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/clipboard_huf8d34ee47161ea8975f770e8f06e63a8_47600_480x0_resize_box_3.png 480w, https://charent.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/clipboard_huf8d34ee47161ea8975f770e8f06e63a8_47600_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;梯度消失示意图&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;梯度爆炸&#34;&gt;梯度爆炸&lt;/h2&gt;
&lt;p&gt;  在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为NaN值，再也无法更新。
通俗来讲就是多个大于1的参数多次连乘，导致结果非常大。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Markdown模板</title>
        <link>https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/</link>
        <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
        
        <guid>https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/</guid>
        <description>&lt;img src="https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/helena-hertz-wWZzXlDpMog-unsplash.jpg" alt="Featured image of post Markdown模板" /&gt;&lt;h2 id=&#34;标题效果&#34;&gt;标题效果&lt;/h2&gt;
&lt;h1 id=&#34;这是一级标题&#34;&gt;这是一级标题&lt;/h1&gt;
&lt;h2 id=&#34;这是二级标题&#34;&gt;这是二级标题&lt;/h2&gt;
&lt;h3 id=&#34;这是三级标题&#34;&gt;这是三级标题&lt;/h3&gt;
&lt;h4 id=&#34;这是四级标题&#34;&gt;这是四级标题&lt;/h4&gt;
&lt;h5 id=&#34;这是五级标题&#34;&gt;这是五级标题&lt;/h5&gt;
&lt;h6 id=&#34;这是六级标题&#34;&gt;这是六级标题&lt;/h6&gt;
&lt;h1 id=&#34;字体效果&#34;&gt;字体效果&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;这是加粗的文字&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;这是倾斜的文字&lt;/em&gt;`&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;这是斜体加粗的文字&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;这是加删除线的文字&lt;/del&gt;&lt;/p&gt;
&lt;h1 id=&#34;段落控制&#34;&gt;段落控制&lt;/h1&gt;
&lt;p&gt;  这是首行缩进&lt;br&gt;
换行：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;br/&amp;gt; 或者 空格 + 空格 + 回车
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;这是引用的内容&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这是引用的内容&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;这是引用的内容&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;分割线&#34;&gt;分割线&lt;/h1&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id=&#34;超链接&#34;&gt;超链接&lt;/h1&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://jianshu.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;简书&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://baidu.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;百度&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;列表&#34;&gt;列表&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;列表内容&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;列表内容&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;列表内容
&lt;ul&gt;
&lt;li&gt;缩进列表，敲3个空格&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;表格&#34;&gt;表格&lt;/h1&gt;
&lt;p&gt;注意：- + * 跟内容之间都要有一个空格&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;表头&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;表头&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;表头&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;内容&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;内容&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;内容&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;内容&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;内容&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;内容&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;第二行分割表头和内容。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有一个就行，为了对齐，多加了几个
文字默认居左
-两边加：表示文字居中
-右边加：表示文字居右
注：原生的语法两边都要用 | 包起来。此处省略&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;latex-公式&#34;&gt;LaTeX 公式&lt;/h1&gt;
&lt;p&gt;$ 表示行内公式($之间不能有空格)：&lt;/p&gt;
&lt;p&gt;质能守恒方程可以用一个很简洁的方程式 $E=mc^2$ 来表达。&lt;/p&gt;
&lt;p&gt;$$ 表示整行公式：&lt;/p&gt;
&lt;p&gt;$$\sum_{i=1}^n a_i=0$$&lt;/p&gt;
&lt;p&gt;$$f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2 $$&lt;/p&gt;
&lt;p&gt;$$\sum^{j-1}&lt;em&gt;{k=0}{\widehat{\gamma}&lt;/em&gt;{kj} z_k}$$&lt;/p&gt;
&lt;h1 id=&#34;代码&#34;&gt;代码&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;单行代码内容&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kd&#34;&gt;function&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;fun&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;         &lt;span class=&#34;nx&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;代码块&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nx&#34;&gt;fun&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;流程图：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;st=&amp;gt;start: 开始
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;op=&amp;gt;operation: My Operation
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cond=&amp;gt;condition: Yes or No?
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;e=&amp;gt;end
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;st-&amp;gt;op-&amp;gt;cond
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cond(yes)-&amp;gt;e
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cond(no)-&amp;gt;op
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;图片&#34;&gt;图片&lt;/h2&gt;
&lt;p&gt;图片alt就是显示在图片下面的文字，相当于对图片内容的解释。
图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/luca-bravo-alS7ewQ41M8-unsplash.jpg&#34;
	width=&#34;1000&#34;
	height=&#34;667&#34;
	srcset=&#34;https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/luca-bravo-alS7ewQ41M8-unsplash_hu0a3f1163de68d0b9471979ebf0ecf11e_32400_480x0_resize_q75_box.jpg 480w, https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/luca-bravo-alS7ewQ41M8-unsplash_hu0a3f1163de68d0b9471979ebf0ecf11e_32400_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Photo by Luca Bravo on Unsplash&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;149&#34;
		data-flex-basis=&#34;359px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/helena-hertz-wWZzXlDpMog-unsplash.jpg&#34;
	width=&#34;1000&#34;
	height=&#34;750&#34;
	srcset=&#34;https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_480x0_resize_q75_box.jpg 480w, https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Photo by Helena Hertz on Unsplash&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;  &lt;img src=&#34;https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/hudai-gayiran-3Od_VKcDEAA-unsplash.jpg&#34;
	width=&#34;667&#34;
	height=&#34;1000&#34;
	srcset=&#34;https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/hudai-gayiran-3Od_VKcDEAA-unsplash_hub241c2a9c7a2caf7e16a2a5bbc7141ff_18711_480x0_resize_q75_box.jpg 480w, https://charent.github.io/p/markdown%E6%A8%A1%E6%9D%BF/hudai-gayiran-3Od_VKcDEAA-unsplash_hub241c2a9c7a2caf7e16a2a5bbc7141ff_18711_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Photo by Hudai Gayiran on Unsplash&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;66&#34;
		data-flex-basis=&#34;160px&#34;
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;![&lt;span class=&#34;nt&#34;&gt;Photo by Florian Klauer on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;florian-klauer-nptLmg6jqDo-unsplash.jpg&lt;/span&gt;)  ![&lt;span class=&#34;nt&#34;&gt;Photo by Luca Bravo on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;luca-bravo-alS7ewQ41M8-unsplash.jpg&lt;/span&gt;) 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;![&lt;span class=&#34;nt&#34;&gt;Photo by Helena Hertz on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;helena-hertz-wWZzXlDpMog-unsplash.jpg&lt;/span&gt;)  ![&lt;span class=&#34;nt&#34;&gt;Photo by Hudai Gayiran on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;hudai-gayiran-3Od_VKcDEAA-unsplash.jpg&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;相册语法来自 &lt;a class=&#34;link&#34; href=&#34;https://typlog.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Typlog&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
